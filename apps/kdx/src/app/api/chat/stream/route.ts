import type { NextRequest } from "next/server";
import { NextResponse } from "next/server";

import { aiStudioRepository, chatRepository } from "@kdx/db/repositories";

export async function POST(request: NextRequest) {
  try {
    console.log("ðŸ”µ [API] POST streaming recebido");

    const {
      chatSessionId,
      content,
      useAgent = true,
      skipUserMessage = false,
    } = await request.json();
    console.log("ðŸŸ¢ [API] Dados recebidos:", {
      chatSessionId,
      content,
      useAgent,
      skipUserMessage,
    });

    if (!chatSessionId || !content) {
      return NextResponse.json(
        { error: "ParÃ¢metros invÃ¡lidos" },
        { status: 400 },
      );
    }

    // Verificar se a sessÃ£o existe
    const session =
      await chatRepository.ChatSessionRepository.findById(chatSessionId);
    if (!session) {
      return NextResponse.json(
        { error: "SessÃ£o nÃ£o encontrada" },
        { status: 404 },
      );
    }

    console.log("ðŸ” [DEBUG] Dados da sessÃ£o:");
    console.log(`   â€¢ ID: ${session.id}`);
    console.log(`   â€¢ TÃ­tulo: ${session.title}`);
    console.log(`   â€¢ aiModelId: ${session.aiModelId || "âŒ NULL/UNDEFINED"}`);
    console.log(`   â€¢ aiAgentId: ${session.aiAgentId || "âŒ NULL/UNDEFINED"}`);
    console.log(`   â€¢ teamId: ${session.teamId}`);

    // Carregar agente se existir na sessÃ£o
    let agent = null;
    if (session.aiAgentId) {
      agent = await aiStudioRepository.AiAgentRepository.findById(
        session.aiAgentId,
      );
      if (agent) {
        console.log(`ðŸ¤– [DEBUG] Agente carregado: ${agent.name}`);
        console.log(
          `ðŸ“ [DEBUG] InstruÃ§Ãµes do agente: ${agent.instructions.substring(0, 100)}...`,
        );
      } else {
        console.log(
          `âŒ [DEBUG] Agente com ID ${session.aiAgentId} nÃ£o encontrado`,
        );
      }
    }

    // âœ… CORREÃ‡ÃƒO: Criar mensagem do usuÃ¡rio apenas se nÃ£o for skipUserMessage
    let userMessage;
    let recentMessages: any[] | null = null;

    if (skipUserMessage) {
      console.log(
        "ðŸ”„ [API] Pulando criaÃ§Ã£o de mensagem do usuÃ¡rio (skipUserMessage=true)",
      );
      // Buscar a mensagem mais recente do usuÃ¡rio com o mesmo conteÃºdo
      recentMessages = await chatRepository.ChatMessageRepository.findBySession(
        {
          chatSessionId: session.id,
          limite: 5,
          offset: 0,
          ordem: "desc",
        },
      );

      userMessage = recentMessages.find(
        (msg: any) => msg.senderRole === "user" && msg.content === content,
      );

      if (!userMessage) {
        console.warn(
          "âš ï¸ [API] Mensagem do usuÃ¡rio nÃ£o encontrada, criando nova",
        );
        userMessage = await chatRepository.ChatMessageRepository.create({
          chatSessionId: session.id,
          senderRole: "user",
          content,
          status: "ok",
        });
      } else {
        console.log("âœ… [API] Mensagem do usuÃ¡rio encontrada:", userMessage.id);
      }
    } else {
      // Comportamento normal: criar nova mensagem do usuÃ¡rio
      userMessage = await chatRepository.ChatMessageRepository.create({
        chatSessionId: session.id,
        senderRole: "user",
        content,
        status: "ok",
      });
      console.log("âœ… [API] Mensagem do usuÃ¡rio criada");
    }

    if (!useAgent) {
      return NextResponse.json({
        userMessage,
        sessionId: session.id,
      });
    }

    // âœ… CORREÃ‡ÃƒO: Buscar histÃ³rico de mensagens ou reutilizar se jÃ¡ carregadas
    let messages;
    if (skipUserMessage && recentMessages) {
      // Se jÃ¡ carregamos mensagens para buscar a do usuÃ¡rio, reutilizar e ordenar
      messages = recentMessages.reverse(); // Inverter para ordem cronolÃ³gica
    } else {
      // Buscar histÃ³rico normalmente
      messages = await chatRepository.ChatMessageRepository.findBySession({
        chatSessionId: session.id,
        limite: 20,
        offset: 0,
        ordem: "asc",
      });
    }

    // âœ… CORREÃ‡ÃƒO: Incluir mensagem do usuÃ¡rio apenas se nÃ£o estiver jÃ¡ incluÃ­da
    const userMessageExists = messages.some(
      (msg: any) => msg.id === userMessage?.id,
    );
    const allMessages = userMessageExists
      ? messages
      : [...messages, userMessage];

    // Formatar mensagens para o formato da OpenAI
    const formattedMessages: { role: string; content: string }[] = [];

    // Detectar idioma do usuÃ¡rio de forma mais robusta
    const detectUserLocale = (request: NextRequest): "pt-BR" | "en" => {
      // 1. Verificar cookie NEXT_LOCALE (usado pelo next-intl)
      const cookieLocale = request.cookies.get("NEXT_LOCALE")?.value;
      if (cookieLocale === "pt-BR" || cookieLocale === "en") {
        return cookieLocale;
      }

      // 2. Verificar pathname do request
      const pathname = request.nextUrl.pathname;
      if (pathname.startsWith("/pt-BR")) return "pt-BR";
      if (pathname.startsWith("/en")) return "en";

      // 3. Verificar Accept-Language header
      const acceptLanguage = request.headers.get("accept-language") || "";
      if (acceptLanguage.includes("pt")) return "pt-BR";
      if (acceptLanguage.includes("en")) return "en";

      // 4. Fallback para portuguÃªs (locale padrÃ£o)
      return "pt-BR";
    };

    const userLocale = detectUserLocale(request);

    // System prompts multilÃ­ngues
    const baseSystemPrompts = {
      "pt-BR":
        "VocÃª Ã© um assistente Ãºtil e responde sempre em portuguÃªs brasileiro. Seja claro, objetivo e mantenha um tom profissional e amigÃ¡vel.",
      en: "You are a helpful assistant and always respond in English. Be clear, objective, and maintain a professional and friendly tone.",
    };

    // Construir system prompt considerando instruÃ§Ãµes do agente
    let systemPrompt =
      baseSystemPrompts[userLocale] || baseSystemPrompts["pt-BR"];

    if (agent?.instructions) {
      console.log("ðŸ¤– [API] Incluindo instruÃ§Ãµes do agente no system prompt");
      systemPrompt = `${agent.instructions}\n\n${systemPrompt}`;
    }

    // Adicionar system prompt no idioma correto se nÃ£o existir
    const hasSystemPrompt = allMessages.some(
      (msg) => msg?.senderRole === "system",
    );
    if (!hasSystemPrompt) {
      formattedMessages.push({
        role: "system",
        content: systemPrompt,
      });
      console.log(
        `ðŸŒ [API] System prompt adicionado em: ${userLocale}${agent ? " (com instruÃ§Ãµes do agente)" : ""}`,
      );
    }

    for (const msg of allMessages) {
      if (msg?.content) {
        formattedMessages.push({
          role:
            msg.senderRole === "user"
              ? "user"
              : msg.senderRole === "system"
                ? "system"
                : "assistant",
          content: msg.content,
        });
      }
    }

    // Buscar modelo e provider
    let model;

    console.log("ðŸŽ¯ [DEBUG] Verificando modelo da sessÃ£o...");
    if (session.aiModelId) {
      console.log(`ðŸ” [DEBUG] Buscando modelo com ID: ${session.aiModelId}`);
      model = await aiStudioRepository.AiModelRepository.findById(
        session.aiModelId,
      );
      if (model) {
        console.log(
          `âœ… [DEBUG] Modelo encontrado: ${model.name} (Provider: ${model.provider.name})`,
        );
      } else {
        console.log(
          `âŒ [DEBUG] Modelo com ID ${session.aiModelId} nÃ£o encontrado no banco`,
        );
      }
    } else {
      console.log("âŒ [DEBUG] SessÃ£o nÃ£o possui aiModelId definido");
    }

    // Se nÃ£o encontrou modelo ou sessÃ£o nÃ£o tem modelo configurado, usar modelo padrÃ£o
    if (!model) {
      console.log(
        "âš ï¸ [API] SessÃ£o sem modelo configurado, buscando modelo padrÃ£o...",
      );

      // Buscar primeiro modelo disponÃ­vel do OpenAI
      const availableModels =
        await aiStudioRepository.AiModelRepository.findMany({
          enabled: true,
          limite: 1,
        });

      console.log(
        `ðŸ” [DEBUG] Modelos disponÃ­veis encontrados: ${availableModels.length}`,
      );

      if (availableModels.length === 0) {
        throw new Error(
          "Nenhum modelo de IA disponÃ­vel. Configure um modelo no AI Studio.",
        );
      }

      model = availableModels[0]!;
      console.log(
        `âš ï¸ [DEBUG] Usando modelo padrÃ£o: ${model.name} (Provider: ${model.provider.name})`,
      );

      // Atualizar a sessÃ£o com o modelo padrÃ£o
      await chatRepository.ChatSessionRepository.update(session.id, {
        aiModelId: model.id,
      });

      console.log(
        `âœ… [API] SessÃ£o atualizada com modelo padrÃ£o: ${model.name}`,
      );
    } else {
      console.log(
        `ðŸŽ¯ [DEBUG] Usando modelo selecionado da sessÃ£o: ${model.name} (Provider: ${model.provider.name})`,
      );
    }

    // Verificar se temos um modelo vÃ¡lido apÃ³s todas as tentativas
    if (!model) {
      throw new Error("NÃ£o foi possÃ­vel obter um modelo vÃ¡lido");
    }

    if (!model.providerId) {
      throw new Error("Modelo nÃ£o possui provider configurado");
    }

    // Verificar se o provider estÃ¡ carregado
    if (!model.provider) {
      throw new Error("Dados do provider nÃ£o foram carregados corretamente");
    }

    // Buscar token do provider
    const providerToken =
      await aiStudioRepository.AiTeamProviderTokenRepository.findByTeamAndProvider(
        session.teamId,
        model.providerId,
      );

    if (!providerToken?.token) {
      throw new Error(
        `Token nÃ£o configurado para o provider ${model.provider.name || "provider"}. Configure um token no AI Studio.`,
      );
    }

    console.log("âœ… [API] ConfiguraÃ§Ãµes obtidas, iniciando streaming...");

    // Configurar API baseada no provider
    const baseUrl = model.provider.baseUrl || "https://api.openai.com/v1";
    const apiUrl = `${baseUrl}/chat/completions`;

    // Usar configuraÃ§Ãµes do modelo
    const modelConfig = (model.config as any) || {};
    const modelName = modelConfig.version || model.name;

    // ValidaÃ§Ã£o inteligente de max_tokens baseada no modelo
    const getMaxTokensForModel = (
      modelName: string,
      configMaxTokens?: number,
    ) => {
      // Limites conhecidos para diferentes modelos da OpenAI
      const modelLimits: Record<string, number> = {
        "gpt-4": 8192,
        "gpt-4-turbo": 4096,
        "gpt-4-turbo-preview": 4096,
        "gpt-4o": 4096,
        "gpt-4o-mini": 16384,
        "gpt-3.5-turbo": 4096,
        "gpt-3.5-turbo-16k": 16384,
      };

      // Encontrar limite baseado no nome do modelo (normalizado)
      const normalizedModelName = modelName
        .toLowerCase()
        .replace(/-\d{4}-\d{2}-\d{2}$/, "")
        .replace(/-\d{4}$/, "");

      const modelLimit = modelLimits[normalizedModelName];

      // Se hÃ¡ configuraÃ§Ã£o no modelo, usar o menor entre config e limite do modelo
      if (configMaxTokens && modelLimit) {
        return Math.min(configMaxTokens, modelLimit);
      }

      // Se sÃ³ hÃ¡ limite do modelo, usar ele
      if (modelLimit) {
        return modelLimit;
      }

      // Se sÃ³ hÃ¡ config, usar no mÃ¡ximo 4096 como padrÃ£o seguro
      if (configMaxTokens) {
        return Math.min(configMaxTokens, 4096);
      }

      // Fallback seguro
      return 1000;
    };

    const maxTokens = getMaxTokensForModel(modelName, modelConfig.maxTokens);
    const temperature = modelConfig.temperature || 0.7;

    console.log(
      `ðŸŸ¢ [API] Usando modelo: ${modelName} (Provider: ${model.provider.name})`,
    );
    console.log(`ðŸŽ¯ [API] Max tokens ajustado: ${maxTokens}`);

    if (!modelName) {
      throw new Error("Nome do modelo nÃ£o configurado corretamente");
    }

    console.log("ðŸŸ¢ [API] Preparando payload para OpenAI");
    console.log(
      `ðŸŽ¯ [API] Modelo: ${modelName}, Mensagens: ${formattedMessages.length}`,
    );

    // Preparar payload para OpenAI
    const payload = {
      model: modelName,
      messages: formattedMessages,
      max_tokens: maxTokens,
      temperature: temperature,
      stream: true,
    };

    // Fazer chamada para API com streaming
    const response = await fetch(apiUrl, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${providerToken.token}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(payload),
    });

    // ðŸ”§ FIX: Tratamento especÃ­fico de erros da OpenAI
    if (!response.ok) {
      const errorText = await response.text();
      console.error(`âŒ [API] OpenAI API Error:`, {
        status: response.status,
        statusText: response.statusText,
        errorText: errorText.substring(0, 500), // Limitar tamanho do log
        modelName,
        baseUrl,
        providerId: model.providerId,
        teamId: session.teamId,
      });

      // Verificar se Ã© erro de autenticaÃ§Ã£o (token invÃ¡lido no banco)
      if (response.status === 401) {
        throw new Error(
          `Token da OpenAI invÃ¡lido ou expirado para este team. Acesse o AI Studio > ConfiguraÃ§Ãµes > Providers e verifique/atualize o token da OpenAI. (Status: ${response.status})`,
        );
      }

      // Verificar se Ã© erro de quota/billing
      if (response.status === 429) {
        throw new Error(
          `Limite de uso da OpenAI excedido. Verifique sua conta OpenAI ou configure um novo token no AI Studio. (Status: ${response.status})`,
        );
      }

      // Verificar se Ã© erro de modelo nÃ£o encontrado
      if (response.status === 404 && errorText.includes("model")) {
        throw new Error(
          `Modelo "${modelName}" nÃ£o encontrado na OpenAI. Verifique se o modelo estÃ¡ disponÃ­vel para sua conta ou configure um modelo diferente no AI Studio. (Status: ${response.status})`,
        );
      }

      // Verificar se Ã© erro de permissÃ£o
      if (response.status === 403) {
        throw new Error(
          `Sem permissÃ£o para usar o modelo "${modelName}" com o token configurado. Verifique suas permissÃµes na OpenAI ou atualize o token no AI Studio. (Status: ${response.status})`,
        );
      }

      // Erro genÃ©rico com orientaÃ§Ã£o especÃ­fica do AI Studio
      throw new Error(
        `Erro na API da OpenAI: ${response.status} - ${response.statusText}. Verifique a configuraÃ§Ã£o do token no AI Studio > ConfiguraÃ§Ãµes > Providers. ${errorText ? `Detalhes: ${errorText.substring(0, 200)}...` : ""}`,
      );
    }

    console.log("ðŸŸ¢ [API] Stream da IA obtido, iniciando transmissÃ£o");

    let fullContent = "";
    let actualModelUsed = ""; // Capturar o modelo real da API

    // Criar ReadableStream para transmitir ao frontend
    const stream = new ReadableStream({
      async start(controller) {
        const reader = response.body?.getReader();
        const decoder = new TextDecoder();

        if (!reader) {
          controller.close();
          return;
        }

        try {
          while (true) {
            const { done, value } = await reader.read();
            if (done) break;

            const chunk = decoder.decode(value);
            const lines = chunk.split("\n");

            for (const line of lines) {
              if (line.startsWith("data: ")) {
                const data = line.slice(6).trim();

                if (data === "[DONE]") {
                  break;
                }

                try {
                  const parsed = JSON.parse(data);

                  // Capturar o modelo real usado pela API
                  if (parsed.model && !actualModelUsed) {
                    actualModelUsed = parsed.model;
                    console.log(
                      `ðŸŽ¯ [API] Modelo real usado pela OpenAI: ${actualModelUsed}`,
                    );
                  }

                  const delta = parsed.choices?.[0]?.delta?.content;

                  if (delta) {
                    fullContent += delta;

                    // Criar encoder para transmitir ao frontend
                    const encoder = new TextEncoder();
                    const encodedChunk = encoder.encode(delta);

                    // Enviar chunk para o frontend
                    controller.enqueue(encodedChunk);
                  }
                } catch (parseError) {
                  // Ignorar erros de parsing
                  continue;
                }
              }
            }
          }

          console.log("ðŸŸ¢ [API] Streaming concluÃ­do, salvando mensagem");

          // Salvar resposta completa da IA no banco
          const safeContent =
            fullContent || "Desculpe, nÃ£o consegui gerar uma resposta.";

          const savedMessage =
            await chatRepository.ChatMessageRepository.create({
              chatSessionId: session.id,
              senderRole: "ai",
              content: safeContent,
              status: "ok",
              metadata: {
                actualModelUsed: actualModelUsed || modelName,
                requestedModel: modelName,
                providerId: model.providerId,
                timestamp: new Date().toISOString(),
              },
            });

          console.log(
            `âœ… [API] Mensagem salva com metadata do modelo: ${actualModelUsed}`,
          );

          controller.close();
        } catch (error) {
          console.error("ðŸ”´ [API] Erro no streaming:", error);
          controller.close();
        } finally {
          reader.releaseLock();
        }
      },
    });

    // Se criou uma nova sessÃ£o, incluir o ID no header
    const headers: HeadersInit = {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
    };

    return new NextResponse(stream, { headers });
  } catch (error) {
    console.error("ðŸ”´ [API] Erro no endpoint de streaming:", error);
    return NextResponse.json(
      {
        error:
          error instanceof Error ? error.message : "Erro interno do servidor",
      },
      { status: 500 },
    );
  }
}
