import type { NextRequest } from "next/server";
import { NextResponse } from "next/server";

import { chatAppId } from "@kdx/shared";

// ðŸš€ Importar VercelAIAdapter para sistema Ãºnico
import { VercelAIAdapter } from "../../../../../../../packages/api/src/internal/adapters/vercel-ai-adapter";
import { AiStudioService } from "../../../../../../../packages/api/src/internal/services/ai-studio.service";
import { ChatService } from "../../../../../../../packages/api/src/internal/services/chat.service";

export async function POST(request: NextRequest) {
  try {
    console.log("ðŸ”µ [API] POST streaming recebido");

    const {
      chatSessionId,
      content,
      useAgent = true,
      skipUserMessage = false,
    } = await request.json();
    console.log("ðŸŸ¢ [API] Dados recebidos:", {
      chatSessionId,
      content,
      useAgent,
      skipUserMessage,
    });

    if (!chatSessionId || !content) {
      return NextResponse.json(
        { error: "ParÃ¢metros invÃ¡lidos" },
        { status: 400 },
      );
    }

    // Verificar se a sessÃ£o existe
    const session = await ChatService.findSessionById(chatSessionId);
    if (!session) {
      return NextResponse.json(
        { error: "SessÃ£o nÃ£o encontrada" },
        { status: 404 },
      );
    }

    console.log("ðŸ” [DEBUG] Dados da sessÃ£o:");
    console.log(`   â€¢ ID: ${session.id}`);
    console.log(`   â€¢ TÃ­tulo: ${session.title}`);
    console.log(`   â€¢ aiModelId: ${session.aiModelId || "âŒ NULL/UNDEFINED"}`);
    console.log(`   â€¢ aiAgentId: ${session.aiAgentId || "âŒ NULL/UNDEFINED"}`);
    console.log(`   â€¢ teamId: ${session.teamId}`);

    // âœ… TEMPORÃRIO: Agente serÃ¡ removido do fluxo de streaming por enquanto
    // TODO: Implementar getAgentById no AiStudioService
    const agent = null;
    if (session.aiAgentId) {
      console.log(
        `âš ï¸ [DEBUG] Agente ${session.aiAgentId} serÃ¡ ignorado no streaming por enquanto`,
      );
    }

    // âœ… CORREÃ‡ÃƒO: Criar mensagem do usuÃ¡rio apenas se nÃ£o for skipUserMessage
    let userMessage;
    let recentMessages: any[] | null = null;

    if (skipUserMessage) {
      console.log(
        "ðŸ”„ [API] Pulando criaÃ§Ã£o de mensagem do usuÃ¡rio (skipUserMessage=true)",
      );
      // Buscar a mensagem mais recente do usuÃ¡rio com o mesmo conteÃºdo
      recentMessages = await ChatService.findMessagesBySession({
        chatSessionId: session.id,
        limite: 5,
        offset: 0,
        ordem: "desc",
      });

      userMessage = recentMessages.find(
        (msg: any) => msg.senderRole === "user" && msg.content === content,
      );

      if (!userMessage) {
        console.warn(
          "âš ï¸ [API] Mensagem do usuÃ¡rio nÃ£o encontrada, criando nova",
        );
        userMessage = await ChatService.createMessage({
          chatSessionId: session.id,
          senderRole: "user",
          content,
          status: "ok",
        });
      } else {
        console.log("âœ… [API] Mensagem do usuÃ¡rio encontrada:", userMessage.id);
      }
    } else {
      // Comportamento normal: criar nova mensagem do usuÃ¡rio
      userMessage = await ChatService.createMessage({
        chatSessionId: session.id,
        senderRole: "user",
        content,
        status: "ok",
      });
      console.log("âœ… [API] Mensagem do usuÃ¡rio criada");
    }

    if (!useAgent) {
      return NextResponse.json({
        userMessage,
        sessionId: session.id,
      });
    }

    // âœ… CORREÃ‡ÃƒO: Buscar histÃ³rico de mensagens ou reutilizar se jÃ¡ carregadas
    let messages;
    if (skipUserMessage && recentMessages) {
      // Se jÃ¡ carregamos mensagens para buscar a do usuÃ¡rio, reutilizar e ordenar
      messages = recentMessages.reverse(); // Inverter para ordem cronolÃ³gica
    } else {
      // Buscar histÃ³rico normalmente
      messages = await ChatService.findMessagesBySession({
        chatSessionId: session.id,
        limite: 20,
        offset: 0,
        ordem: "asc",
      });
    }

    // âœ… CORREÃ‡ÃƒO: Incluir mensagem do usuÃ¡rio apenas se nÃ£o estiver jÃ¡ incluÃ­da
    const userMessageExists = messages.some(
      (msg: any) => msg.id === userMessage?.id,
    );
    const allMessages = userMessageExists
      ? messages
      : [...messages, userMessage];

    // ðŸš€ SISTEMA VERCEL AI SDK (Ãºnico sistema)
    console.log("ðŸš€ [VERCEL_AI] Usando Vercel AI SDK");

    try {
      // Detectar idioma do usuÃ¡rio de forma mais robusta
      const detectUserLocale = (request: NextRequest): "pt-BR" | "en" => {
        const cookieLocale = request.cookies.get("NEXT_LOCALE")?.value;
        if (cookieLocale === "pt-BR" || cookieLocale === "en") {
          return cookieLocale;
        }
        const pathname = request.nextUrl.pathname;
        if (pathname.startsWith("/pt-BR")) return "pt-BR";
        if (pathname.startsWith("/en")) return "en";
        const acceptLanguage = request.headers.get("accept-language") || "";
        if (acceptLanguage.includes("pt")) return "pt-BR";
        if (acceptLanguage.includes("en")) return "en";
        return "pt-BR";
      };

      // Verificar se hÃ¡ Team Instructions na sessÃ£o
      const hasTeamInstructions = allMessages.some(
        (msg) =>
          msg?.senderRole === "system" &&
          msg?.metadata?.type === "team_instructions",
      );

      // System prompt baseado no idioma do usuÃ¡rio
      const userLocale = detectUserLocale(request);
      const systemPrompt =
        userLocale === "pt-BR"
          ? "VocÃª Ã© um assistente Ãºtil e responde sempre em portuguÃªs brasileiro."
          : "You are a helpful assistant and always respond in English.";

      // Preparar mensagens para o adapter
      const formattedMessages: {
        senderRole: "user" | "ai" | "system";
        content: string;
      }[] = [];

      // SÃ³ adicionar system prompt se nÃ£o hÃ¡ Team Instructions
      if (!hasTeamInstructions) {
        const hasSystemPrompt = allMessages.some(
          (msg) => msg?.senderRole === "system",
        );
        if (!hasSystemPrompt) {
          formattedMessages.push({
            senderRole: "system",
            content: systemPrompt,
          });
          console.log(`ðŸŒ [API] System prompt adicionado em: ${userLocale}`);
        }
      } else {
        console.log(
          `ðŸŽ¯ [API] Team Instructions detectadas, pulando system prompt padrÃ£o`,
        );
      }

      // Adicionar todas as mensagens existentes
      allMessages.forEach((msg: any) => {
        formattedMessages.push({
          senderRole: msg.senderRole,
          content: msg.content,
        });
      });

      // Buscar modelo da sessÃ£o ou usar padrÃ£o
      let model;
      if (session.aiModelId) {
        try {
          model = await AiStudioService.getModelById({
            modelId: session.aiModelId,
            teamId: session.teamId,
            requestingApp: chatAppId,
          });
        } catch (error) {
          console.log(
            `âŒ [DEBUG] Modelo com ID ${session.aiModelId} nÃ£o encontrado:`,
            error,
          );
        }
      }

      // Se nÃ£o encontrou modelo, usar modelo padrÃ£o
      if (!model) {
        console.log(
          "âš ï¸ [API] SessÃ£o sem modelo configurado, buscando modelo padrÃ£o...",
        );
        const availableModels = await AiStudioService.getAvailableModels({
          teamId: session.teamId,
          requestingApp: chatAppId,
        });

        if (availableModels.length === 0) {
          throw new Error(
            "Nenhum modelo de IA disponÃ­vel. Configure um modelo no AI Studio.",
          );
        }

        model = availableModels[0]!;
        console.log(`âš ï¸ [DEBUG] Usando modelo padrÃ£o: ${model.name}`);

        await ChatService.updateSession(session.id, {
          aiModelId: model.id,
        });
      }

      // Criar adapter e processar streaming com auto-save
      const adapter = new VercelAIAdapter();
      const response = await adapter.streamAndSave(
        {
          chatSessionId: session.id,
          content,
          modelId: model.id,
          teamId: session.teamId,
          messages: formattedMessages,
        },
        async (content: string, metadata: any) => {
          // Callback para salvar mensagem da IA
          await ChatService.createMessage({
            chatSessionId: session.id,
            senderRole: "ai",
            content,
            status: "ok",
            metadata,
          });
        },
      );

      // Interface ultra-limpa: apenas retornar o stream
      const headers: HeadersInit = {
        "Content-Type": "text/plain; charset=utf-8",
        "Cache-Control": "no-cache",
        "X-Powered-By": "Vercel-AI-SDK", // Identificar que estÃ¡ usando o Vercel AI SDK
      };

      return new NextResponse(response.stream, { headers });
    } catch (error) {
      console.error("ðŸ”´ [VERCEL_AI] Erro no Vercel AI SDK:", error);
      throw error; // Re-throw para ser capturado pelo catch geral
    }
  } catch (error) {
    console.error("ðŸ”´ [API] Erro no endpoint de streaming:", error);
    return NextResponse.json(
      {
        error:
          error instanceof Error ? error.message : "Erro interno do servidor",
      },
      { status: 500 },
    );
  }
}
