# Chat Preferred Model Endpoint - Kodix

## ğŸ“– Overview

O endpoint **`getPreferredModel`** implementa uma **hierarquia de prioridade inteligente** para determinar qual modelo de IA usar no chat, **respeitando o isolamento total entre subapps** via chamadas HTTP.

## ğŸ—ï¸ **Arquitetura de Isolamento**

### âœ… **Subapp Isolation Compliance**

- **Chat** âŒ NÃƒO acessa repositÃ³rios do AI Studio diretamente
- **Chat** âœ… Faz chamadas HTTP para endpoints do AI Studio
- **AI Studio** âœ… ExpÃµe endpoints HTTP especÃ­ficos para Chat
- **Isolamento Total** âœ… Mantido entre subapps

### ğŸ”„ **Fluxo de ComunicaÃ§Ã£o**

```
Chat Router â†’ HTTP Request â†’ AI Studio HTTP API â†’ AI Studio Repository â†’ Database
```

## ğŸ¯ **LÃ³gica de Prioridade**

### **1Âª Prioridade: Chat Team Config**

- **Source**: `lastSelectedModelId` da configuraÃ§Ã£o do team no Chat
- **Quando usar**: Quando o team jÃ¡ selecionou um modelo especÃ­fico no chat
- **PersistÃªncia**: Salvo automaticamente quando usuÃ¡rio seleciona modelo

### **2Âª Prioridade: AI Studio Default**

- **Source**: Modelo marcado como `isDefault: true` no AI Studio
- **Quando usar**: Quando nÃ£o hÃ¡ modelo salvo no Chat Team Config
- **ConfiguraÃ§Ã£o**: Definido pelos admins no AI Studio
- **API Call**: `GET /api/ai-studio/chat-integration?action=getDefaultModel`

### **3Âª Prioridade: Primeiro Modelo Ativo**

- **Source**: Primeiro modelo habilitado (`enabled: true`) disponÃ­vel
- **Quando usar**: Como fallback final
- **CritÃ©rio**: Primeiro da lista de modelos ativos do team
- **API Call**: `GET /api/ai-studio/chat-integration?action=getAvailableModels`

## ğŸ› ï¸ **ImplementaÃ§Ã£o Backend**

### **Endpoints HTTP do AI Studio**

```typescript
// apps/kdx/src/app/api/ai-studio/chat-integration/route.ts

export async function GET(request: NextRequest) {
  const { searchParams } = new URL(request.url);
  const action = searchParams.get("action");

  switch (action) {
    case "getModel":
      // Busca modelo por ID
      return aiStudioRepository.AiModelRepository.findById(modelId);

    case "getDefaultModel":
      // Busca modelo padrÃ£o do team
      return aiStudioRepository.AiTeamModelConfigRepository.getDefaultModel(
        teamId,
      );

    case "getAvailableModels":
      // Lista modelos disponÃ­veis do team
      return aiStudioRepository.AiTeamModelConfigRepository.findAvailableModelsByTeam(
        teamId,
      );

    case "getProviderToken":
      // Busca token do provider
      return aiStudioRepository.AiTeamProviderTokenRepository.findByTeamAndProvider(
        teamId,
        providerId,
      );
  }
}
```

### **Chat Router com HTTP Calls**

```typescript
// packages/api/src/trpc/routers/app/chat/_router.ts

// Helper para chamadas HTTP respeitando isolamento
async function callAiStudioEndpoint(
  action: string,
  params?: Record<string, string>,
  headers?: Record<string, string>,
): Promise<any> {
  const baseUrl = process.env.KODIX_API_URL || "http://localhost:3000";
  const searchParams = new URLSearchParams({ action, ...(params || {}) });
  const url = `${baseUrl}/api/ai-studio/chat-integration?${searchParams.toString()}`;

  const response = await fetch(url, {
    method: "GET",
    headers: { "Content-Type": "application/json", ...headers },
  });

  const result = (await response.json()) as {
    success: boolean;
    data?: any;
    error?: string;
  };
  if (!result.success) {
    throw new Error(`AI Studio Error: ${result.error}`);
  }

  return result.data;
}

getPreferredModel: protectedProcedure.query(async ({ ctx }) => {
  // 1Âª Prioridade: Chat Team Config
  const chatConfigs = await appRepository.findAppTeamConfigs({
    appId: chatAppId,
    teamIds: [teamId],
  });

  const lastSelectedModelId = chatConfig?.config?.lastSelectedModelId;
  if (lastSelectedModelId) {
    // ğŸ”„ HTTP Call respeitando isolamento
    const model = await callAiStudioEndpoint(
      "getModel",
      { modelId: lastSelectedModelId },
      { Authorization: ctx.token || "" },
    );

    if (model) {
      return {
        source: "chat_config",
        modelId: model.id,
        model,
        config: chatConfig?.config,
      };
    }
  }

  // 2Âª Prioridade: AI Studio Default via HTTP
  const defaultModelConfig = await callAiStudioEndpoint(
    "getDefaultModel",
    undefined,
    { Authorization: ctx.token || "" },
  );

  if (defaultModelConfig?.model) {
    return {
      source: "ai_studio_default",
      modelId: defaultModelConfig.model.id,
      model: defaultModelConfig.model,
      teamConfig: defaultModelConfig,
    };
  }

  // 3Âª Prioridade: Primeiro modelo ativo via HTTP
  const availableModels = await callAiStudioEndpoint(
    "getAvailableModels",
    undefined,
    { Authorization: ctx.token || "" },
  );

  const firstActiveModel = (availableModels || []).find(
    (m: any) => m.teamConfig?.enabled,
  );

  if (firstActiveModel) {
    return {
      source: "first_available",
      modelId: firstActiveModel.id,
      model: firstActiveModel,
      teamConfig: firstActiveModel.teamConfig,
    };
  }

  throw new TRPCError({
    code: "NOT_FOUND",
    message: "Nenhum modelo de IA disponÃ­vel. Configure modelos no AI Studio.",
  });
});
```

### **Estrutura de Resposta**

```typescript
interface PreferredModelResponse {
  source: "chat_config" | "ai_studio_default" | "first_available";
  modelId: string;
  model: {
    id: string;
    name: string;
    providerId: string;
    config: any;
    provider: {
      name: string;
      baseUrl?: string;
    };
  };
  config?: any; // Chat config quando source = "chat_config"
  teamConfig?: any; // AI Studio config quando source = "ai_studio_default" | "first_available"
}
```

## ğŸ¯ **Frontend Integration**

### **Hook personalizado**

```typescript
// apps/kdx/src/app/[locale]/(authed)/apps/chat/_hooks/useChatPreferredModel.ts

export function useChatPreferredModel() {
  const trpc = useTRPC();

  const {
    data: preferredModel,
    isLoading,
    error,
    refetch,
  } = useQuery({
    ...trpc.app.chat.getPreferredModel.queryOptions(),
    staleTime: 5 * 60 * 1000, // Cache por 5 minutos
    refetchOnWindowFocus: false,
  });

  return {
    preferredModel,
    isLoading,
    error,
    refetch,

    // Helpers
    modelId: preferredModel?.modelId,
    model: preferredModel?.model,
    source: preferredModel?.source,

    // VerificaÃ§Ãµes
    isFromChatConfig: preferredModel?.source === "chat_config",
    isFromAiStudio: preferredModel?.source === "ai_studio_default",
    isFallback: preferredModel?.source === "first_available",
  };
}
```

### **Exemplo de uso**

```typescript
// No componente do Chat
const { preferredModel, isLoading, isFromChatConfig } = useChatPreferredModel();

if (isLoading) return <Loading />;

return (
  <div>
    <h3>Modelo Atual: {preferredModel?.model?.name}</h3>
    <Badge variant={isFromChatConfig ? "primary" : "secondary"}>
      {isFromChatConfig ? "Salvo no Chat" : "PadrÃ£o do AI Studio"}
    </Badge>
  </div>
);
```

## âœ… **BenefÃ­cios da Arquitetura**

### **ğŸ”’ Isolamento Garantido**

- Subapps nÃ£o acessam repositÃ³rios uns dos outros
- ComunicaÃ§Ã£o apenas via HTTP APIs
- Facilita manutenÃ§Ã£o e evoluÃ§Ã£o independente

### **ğŸš€ Performance**

- Cache inteligente no frontend (5 min)
- Chamadas HTTP otimizadas
- Fallbacks eficientes

### **ğŸ›¡ï¸ SeguranÃ§a**

- AutenticaÃ§Ã£o em cada chamada HTTP
- ValidaÃ§Ã£o de team ownership
- Error handling robusto

### **ğŸ“ˆ Escalabilidade**

- Subapps podem ser deployed independentemente
- APIs HTTP podem ser facilmente monitoradas
- Facilita implementaÃ§Ã£o de rate limiting

## ğŸ§ª **Testes**

```bash
# Testar endpoint (requer autenticaÃ§Ã£o)
curl -H "Authorization: Bearer <token>" \
  "http://localhost:3000/api/ai-studio/chat-integration?action=getDefaultModel"

# Testar via tRPC no frontend
const { preferredModel } = useChatPreferredModel();
console.log("Modelo preferido:", preferredModel);
```

## ğŸ“Š **Monitoramento**

Os logs incluem informaÃ§Ãµes detalhadas sobre:

- Qual prioridade foi usada
- Tempos de resposta das chamadas HTTP
- Erros e fallbacks acionados
- Team e modelo selecionado

```
ğŸ¯ [PREFERRED_MODEL] Buscando modelo preferido para team: team_123
âœ… [PREFERRED_MODEL] Encontrado lastSelectedModelId: model_456
ğŸ”„ [AI_STUDIO_API] Chat integration request: getModel
âœ… [PREFERRED_MODEL] Modelo encontrado: gpt-4-turbo
```

## ğŸ¯ **Vantagens da SoluÃ§Ã£o**

### **ğŸ§  InteligÃªncia**

- Combina configuraÃ§Ãµes de duas fontes
- Prioridade baseada na experiÃªncia do usuÃ¡rio
- Fallbacks robustos

### **ğŸ”„ Continuidade**

- Preserva escolhas anteriores do team
- Respeita configuraÃ§Ãµes dos admins
- ExperiÃªncia consistente

### **âš¡ Performance**

- Cache de 5 minutos
- Uma Ãºnica query otimizada
- Fallbacks sem queries adicionais

### **ğŸ›¡ï¸ Robustez**

- Funciona mesmo com configuraÃ§Ãµes incompletas
- Limpa automaticamente dados invÃ¡lidos
- Logs detalhados para debugging

## ğŸ“Š **Exemplos de Logs**

### **Modelo do Chat Config**

```
ğŸ¯ [PREFERRED_MODEL] Buscando modelo preferido para team: abc123
âœ… [PREFERRED_MODEL] Encontrado lastSelectedModelId: gpt-4o-2024-11-20
âœ… [PREFERRED_MODEL] Modelo encontrado: GPT-4o
```

### **Modelo PadrÃ£o do AI Studio**

```
ğŸ¯ [PREFERRED_MODEL] Buscando modelo preferido para team: abc123
ğŸ”„ [PREFERRED_MODEL] Buscando modelo padrÃ£o do AI Studio
âœ… [PREFERRED_MODEL] Modelo padrÃ£o do AI Studio encontrado: Claude 3.5 Sonnet
```

### **Fallback para Primeiro Ativo**

```
ğŸ¯ [PREFERRED_MODEL] Buscando modelo preferido para team: abc123
ğŸ”„ [PREFERRED_MODEL] Buscando modelo padrÃ£o do AI Studio
âš ï¸ [PREFERRED_MODEL] Nenhum modelo padrÃ£o configurado no AI Studio
ğŸ”„ [PREFERRED_MODEL] Usando primeiro modelo ativo como fallback: GPT-3.5 Turbo
```

## ğŸ§ª **Como Testar**

### **1. Teste de Prioridades**

```typescript
// Teste manual das prioridades

// 1. Limpar Chat Team Config
// 2. Configurar modelo padrÃ£o no AI Studio
// 3. Verificar se retorna modelo do AI Studio

// 4. Usar chat e selecionar modelo diferente
// 5. Verificar se prÃ³xima chamada retorna lastSelectedModelId
```

### **2. Teste de Fallbacks**

```typescript
// 1. Remover modelo padrÃ£o do AI Studio
// 2. Verificar se usa primeiro modelo ativo

// 3. Desabilitar todos os modelos
// 4. Verificar se retorna erro apropriado
```

## ğŸ”— **IntegraÃ§Ã£o com Funcionalidades Existentes**

### **Chat Team Config System**

- âœ… LÃª `lastSelectedModelId` existente
- âœ… CompatÃ­vel com salvamento automÃ¡tico
- âœ… Respeita configuraÃ§Ãµes por team

### **AI Studio**

- âœ… Usa modelo padrÃ£o configurado
- âœ… Respeita modelos habilitados/desabilitados
- âœ… Considera tokens de provider

### **Model Selector**

- âœ… PrÃ©-seleciona modelo inteligentemente
- âœ… Atualiza Chat Team Config automaticamente
- âœ… Feedback visual da fonte do modelo

---

## ğŸ“ **ConclusÃ£o**

O endpoint `getPreferredModel` oferece uma experiÃªncia inteligente e robusta para seleÃ§Ã£o de modelos de IA, combinando o melhor dos dois sistemas:

- **PersonalizaÃ§Ã£o por Team** (Chat Team Config)
- **ConfiguraÃ§Ã£o Administrativa** (AI Studio)
- **Fallbacks Inteligentes** (Primeiro modelo disponÃ­vel)

Isso garante que o chat sempre funcione com um modelo apropriado, respeitando as preferÃªncias do team e configuraÃ§Ãµes dos administradores.
