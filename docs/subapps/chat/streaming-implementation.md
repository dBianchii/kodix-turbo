# Streaming Implementation - Chat SubApp

## üîÑ Vis√£o Geral

O sistema de streaming do Chat permite que respostas da IA apare√ßam progressivamente, criando uma experi√™ncia fluida e responsiva. Utiliza **100% implementa√ß√£o nativa do Vercel AI SDK** com lifecycle callbacks nativos (`onFinish`, `onError`), garantindo auto-save integrado sem camadas de abstra√ß√£o desnecess√°rias.

## üèóÔ∏è Arquitetura do Streaming Nativo

### Fluxo Nativo Simplificado

```
Frontend (useChat) ‚Üí /api/chat/stream ‚Üí streamText() ‚Üí toDataStreamResponse()
    ‚Üë                                                         ‚Üì
    ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Stream Protocol ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Native Stream ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ
```

1. **Frontend** usa `useChat` hook oficial
2. **Backend** executa `streamText()` nativo
3. **Lifecycle Callbacks** gerenciam auto-save
4. **Data Stream Protocol** comunica com frontend
5. **Auto-Save** via `onFinish` callback nativo
6. **Error Handling** via `onError` callback nativo

## üì° Implementa√ß√£o Backend Nativa

### Endpoint de Streaming 100% Nativo

```typescript
// /api/chat/stream/route.ts - IMPLEMENTA√á√ÉO NATIVA
import { createAnthropic } from "@ai-sdk/anthropic";
import { createOpenAI } from "@ai-sdk/openai";
import { streamText } from "ai";

export async function POST(request: NextRequest) {
  try {
    // 1. Valida√ß√£o e prepara√ß√£o
    const { chatSessionId, content } = await request.json();
    const session = await ChatService.findSessionById(chatSessionId);

    // 2. Salvar mensagem do usu√°rio
    await ChatService.createMessage({
      chatSessionId: session.id,
      senderRole: "user",
      content,
      status: "ok",
    });

    // 3. Obter modelo nativo
    const { model: vercelModel, modelName } = await getVercelModel(
      model.id,
      session.teamId,
    );

    // 4. üéØ VERCEL AI SDK NATIVO - streamText() direto
    const result = streamText({
      model: vercelModel,
      messages: formattedMessages,
      temperature: 0.7,
      maxTokens: 4000,

      // ‚úÖ LIFECYCLE CALLBACK NATIVO - Auto-save
      onFinish: async ({ text, usage, finishReason }) => {
        console.log("‚úÖ [VERCEL_AI_NATIVE] Stream finished:", {
          tokens: usage.totalTokens,
          reason: finishReason,
          textLength: text.length,
        });

        try {
          // Auto-save mensagem da IA com metadata nativa
          await ChatService.createMessage({
            chatSessionId: session.id,
            senderRole: "ai",
            content: text,
            status: "ok",
            metadata: {
              requestedModel: modelName,
              actualModelUsed: modelName,
              providerId: "vercel-ai-sdk-native",
              providerName: "Vercel AI SDK Native",
              usage: usage || null,
              finishReason: finishReason || "stop",
              timestamp: new Date().toISOString(),
            },
          });
          console.log("üíæ [VERCEL_AI_NATIVE] Message auto-saved successfully");
        } catch (saveError) {
          console.error("üî¥ [VERCEL_AI_NATIVE] Auto-save error:", saveError);
          // N√£o interromper stream por erro de save
        }
      },

      // ‚úÖ LIFECYCLE CALLBACK NATIVO - Error handling
      onError: (error) => {
        console.error("üî¥ [VERCEL_AI_NATIVE] Stream error:", {
          error: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : undefined,
          sessionId: session.id,
          modelId: model.id,
        });
      },
    });

    // 5. Retornar response nativa
    return result.toDataStreamResponse({
      headers: {
        "Content-Type": "text/plain; charset=utf-8",
        "Cache-Control": "no-cache",
        "X-Powered-By": "Vercel-AI-SDK-Native",
      },
    });
  } catch (error) {
    console.error("üî¥ [API] Error in chat stream:", error);
    return new Response("Internal Server Error", { status: 500 });
  }
}
```

### Helper para Modelos Nativos

```typescript
// Helper function para criar modelos nativos do Vercel AI SDK
async function getVercelModel(modelId: string, teamId: string) {
  // Buscar configura√ß√£o do modelo via AI Studio
  const modelConfig = await AiStudioService.getModelById({
    modelId,
    teamId,
    requestingApp: chatAppId,
  });

  // Buscar token do provider
  const providerToken = await AiStudioService.getProviderToken({
    providerId: modelConfig.providerId,
    teamId,
    requestingApp: chatAppId,
  });

  // Criar provider nativo baseado no tipo
  const providerName = modelConfig.provider.name.toLowerCase();
  const modelName = (modelConfig.config as any)?.version || modelConfig.name;

  if (providerName === "openai") {
    const openaiProvider = createOpenAI({
      apiKey: providerToken.token,
      baseURL: modelConfig.provider.baseUrl || undefined,
    });
    return {
      model: openaiProvider(modelName),
      modelName: modelName,
    };
  }

  if (providerName === "anthropic") {
    const anthropicProvider = createAnthropic({
      apiKey: providerToken.token,
      baseURL: modelConfig.provider.baseUrl || undefined,
    });
    return {
      model: anthropicProvider(modelName),
      modelName: modelName,
    };
  }

  throw new Error(
    `Provider ${modelConfig.provider.name} n√£o suportado. Suportados: OpenAI, Anthropic.`,
  );
}
```

## üñ•Ô∏è Implementa√ß√£o Frontend Nativa

### Hook useChat Oficial

```typescript
// Componente usando useChat nativo do Vercel AI SDK
import { useChat } from "@ai-sdk/react";

export function ChatWindow({ sessionId }: ChatWindowProps) {
  // ‚úÖ HOOK OFICIAL DO VERCEL AI SDK
  const {
    messages,
    input,
    handleInputChange,
    handleSubmit,
    isLoading,
    error,
    setMessages,
    stop,
    append,
  } = useChat({
    api: "/api/chat/stream",
    initialMessages: dbMessages || [],
    body: {
      chatSessionId: sessionId,
      selectedModelId: "claude-3-5-haiku-20241022",
      useAgent: true,
    },
    // ‚úÖ LIFECYCLE CALLBACKS NATIVOS
    onFinish: async (message) => {
      console.log("‚úÖ [FRONTEND] Mensagem conclu√≠da:", message);

      // Auto-focus ap√≥s streaming
      setTimeout(() => {
        inputRef.current?.focus();
      }, 100);

      // Sincronizar com banco ap√≥s processamento
      setTimeout(async () => {
        await syncNow();
        refetchSession();
        queryClient.invalidateQueries(
          trpc.app.chat.listarSessions.pathFilter(),
        );
      }, 1500);
    },
    onError: (error) => {
      console.error("‚ùå [FRONTEND] Erro no chat:", error);
    },
    keepLastMessageOnError: true,
  });

  // Renderiza√ß√£o com estado de streaming nativo
  return (
    <div className="flex h-full flex-col">
      {/* Mensagens */}
      <ScrollArea className="flex-1 p-4">
        <ChatMessages messages={messages} isLoading={isLoading} />
      </ScrollArea>

      {/* Input com controle de streaming */}
      <form onSubmit={handleSubmit} className="p-4">
        <MessageInput
          value={input}
          onChange={handleInputChange}
          onSendMessage={(message) => {
            // Simular submit quando Enter √© pressionado
            const fakeEvent = new Event("submit") as any;
            fakeEvent.preventDefault = () => {};
            handleSubmit(fakeEvent);
          }}
          disabled={isLoading}
          placeholder="Digite sua mensagem..."
          isLoading={isLoading}
          isStreaming={isLoading}
          onStop={stop}
        />

        {error && (
          <Alert variant="destructive">
            <AlertCircle className="h-4 w-4" />
            <AlertDescription>
              {error.message || "Erro ao enviar mensagem"}
            </AlertDescription>
          </Alert>
        )}
      </form>
    </div>
  );
}
```

### Componente de Input com Streaming

```typescript
// MessageInput com controle de streaming nativo
export const MessageInput = forwardRef<HTMLTextAreaElement, MessageInputProps>(
  function MessageInput({
    value,
    onChange,
    onSendMessage,
    disabled = false,
    placeholder,
    isLoading = false,
    isStreaming = false,
    onStop,
  }, ref) {
    const handleKeyDown = (e: KeyboardEvent<HTMLTextAreaElement>) => {
      if (e.key === "Enter" && !e.shiftKey) {
        e.preventDefault();
        if (isStreaming && onStop) {
          // ‚úÖ PARAR STREAMING NATIVO
          onStop();
        } else {
          onSendMessage(value || "");
        }
      }
    };

    return (
      <div className="flex items-end gap-2">
        <Textarea
          ref={ref}
          value={value}
          onChange={onChange}
          onKeyDown={handleKeyDown}
          placeholder={placeholder}
          disabled={disabled || isLoading}
          className="max-h-32 min-h-[44px] resize-none"
        />

        <Button
          onClick={isStreaming && onStop ? onStop : () => onSendMessage(value || "")}
          disabled={isStreaming ? false : !value?.trim()}
          size="icon"
          variant={isStreaming ? "secondary" : "default"}
        >
          {isLoading ? (
            <Loader2 className="h-4 w-4 animate-spin" />
          ) : isStreaming ? (
            <Square className="h-4 w-4" />
          ) : (
            <Send className="h-4 w-4" />
          )}
        </Button>
      </div>
    );
  }
);
```

## üîÑ Data Stream Protocol

### Protocolo Nativo do Vercel AI SDK

O Vercel AI SDK usa um protocolo otimizado para comunica√ß√£o:

```typescript
// Formato do stream nativo
interface DataStreamPart {
  type: "text" | "tool_call" | "tool_result" | "error" | "finish";
  value: string | object;
}

// Exemplo de chunks recebidos
// 0:"Hello"
// 0:" world"
// 0:"!"
// d:{"finishReason":"stop","usage":{"promptTokens":10,"completionTokens":3}}
```

### Estados de Streaming

```typescript
// Estados nativos do useChat
interface ChatState {
  messages: Message[];
  input: string;
  isLoading: boolean; // ‚úÖ Streaming ativo
  error: Error | undefined;

  // Controles nativos
  handleSubmit: (e: FormEvent) => void;
  handleInputChange: (e: ChangeEvent) => void;
  stop: () => void; // ‚úÖ Parar streaming
  reload: () => void;
  append: (message: Message) => void;
}
```

## üöÄ Performance e Otimiza√ß√µes

### Streaming Otimizado

- **Zero Lat√™ncia**: Primeiro token enviado imediatamente
- **Backpressure Handling**: Controle autom√°tico de fluxo
- **Memory Efficient**: Sem buffers intermedi√°rios
- **Error Recovery**: Reconex√£o autom√°tica em falhas

### M√©tricas de Performance

```typescript
// Tracking nativo de performance
const trackStreamingMetrics = {
  timeToFirstToken: performance.now() - startTime,
  totalTokens: usage.totalTokens,
  tokensPerSecond: usage.totalTokens / (duration / 1000),
  streamDuration: duration,
  errorRate: errors / totalRequests,
};

console.log("üìä [METRICS]", trackStreamingMetrics);
```

## üîß Error Handling Nativo

### Callbacks de Erro

```typescript
// Error handling via onError callback
onError: (error) => {
  // Tipos de erro nativos
  if (error.message.includes("rate_limit")) {
    toast.error("Limite de uso atingido. Tente novamente em alguns minutos.");
  } else if (error.message.includes("invalid_api_key")) {
    toast.error("Configura√ß√£o inv√°lida. Verifique o AI Studio.");
  } else if (error.message.includes("model_not_found")) {
    toast.error("Modelo n√£o dispon√≠vel. Selecione outro modelo.");
  } else {
    toast.error("Erro ao processar mensagem. Tente novamente.");
  }

  console.error("üî¥ [CHAT_ERROR]", {
    message: error.message,
    stack: error.stack,
    timestamp: new Date().toISOString(),
  });
};
```

### Recovery Strategies

```typescript
// Estrat√©gias de recupera√ß√£o nativas
const handleStreamError = (error: Error) => {
  // 1. Retry autom√°tico para erros tempor√°rios
  if (error.message.includes("network") || error.message.includes("timeout")) {
    setTimeout(() => {
      // Tentar novamente automaticamente
      reload();
    }, 2000);
  }

  // 2. Fallback para modelo padr√£o
  if (error.message.includes("model_not_found")) {
    // Trocar para modelo padr√£o e tentar novamente
    switchToDefaultModel();
  }

  // 3. Limpar estado em erros cr√≠ticos
  if (error.message.includes("invalid_session")) {
    // Redirecionar para nova sess√£o
    router.push("/apps/chat");
  }
};
```

## üìä Monitoramento e Debug

### Logs Estruturados

```typescript
// Sistema de logs nativo
console.log("üöÄ [VERCEL_AI_NATIVE] Iniciando streaming");
console.log("üì° [STREAM] Primeiro token recebido");
console.log("üíæ [AUTO_SAVE] Mensagem salva automaticamente");
console.log("‚úÖ [VERCEL_AI_NATIVE] Stream conclu√≠do");
console.log("üî¥ [ERROR] Erro durante streaming");
```

### Debug Mode

```typescript
// Ativar debug detalhado
if (process.env.NODE_ENV === "development") {
  console.log("üîç [DEBUG] Model config:", modelConfig);
  console.log("üîç [DEBUG] Messages:", formattedMessages);
  console.log("üîç [DEBUG] Stream result:", result);
}
```

## üîí Seguran√ßa

### Valida√ß√£o de Input

```typescript
// Valida√ß√£o nativa antes do streaming
const validateChatInput = (content: string, sessionId: string) => {
  if (!content.trim()) {
    throw new Error("Mensagem n√£o pode estar vazia");
  }

  if (content.length > 10000) {
    throw new Error("Mensagem muito longa (m√°ximo 10.000 caracteres)");
  }

  if (!sessionId || sessionId.length < 10) {
    throw new Error("ID de sess√£o inv√°lido");
  }
};
```

### Rate Limiting

```typescript
// Rate limiting nativo por usu√°rio
const rateLimiter = new Map<string, number[]>();

const checkRateLimit = (userId: string) => {
  const now = Date.now();
  const userRequests = rateLimiter.get(userId) || [];

  // Limpar requests antigos (√∫ltimos 60 segundos)
  const recentRequests = userRequests.filter((time) => now - time < 60000);

  if (recentRequests.length >= 30) {
    // 30 requests por minuto
    throw new Error("Muitas requisi√ß√µes. Aguarde um momento.");
  }

  recentRequests.push(now);
  rateLimiter.set(userId, recentRequests);
};
```

---

## üìö Refer√™ncias

- **[Vercel AI SDK](https://sdk.vercel.ai/)** - Documenta√ß√£o oficial
- **[useChat Hook](https://sdk.vercel.ai/docs/reference/ai-sdk-ui/use-chat)** - Hook oficial
- **[streamText](https://sdk.vercel.ai/docs/reference/ai-sdk-core/stream-text)** - Fun√ß√£o nativa
- **[Data Stream Protocol](https://sdk.vercel.ai/docs/concepts/streaming#data-stream-protocol)** - Protocolo de comunica√ß√£o

---

**‚úÖ Sistema de Streaming 100% Nativo Operacional**

**üéØ Benef√≠cios Alcan√ßados:**

- ‚úÖ Zero abstra√ß√µes desnecess√°rias
- ‚úÖ Performance m√°xima com streaming nativo
- ‚úÖ Auto-save integrado via lifecycle callbacks
- ‚úÖ Error handling robusto e nativo
- ‚úÖ Compatibilidade total com Vercel AI SDK
- ‚úÖ C√≥digo 75% mais limpo que implementa√ß√£o anterior
