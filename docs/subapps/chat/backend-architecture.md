# Backend Architecture - Chat SubApp

## âš™ï¸ VisÃ£o Geral

O backend do Chat Ã© construÃ­do com tRPC para APIs type-safe e **implementaÃ§Ã£o 100% nativa do Vercel AI SDK**. A arquitetura utiliza `streamText()` direto com lifecycle callbacks nativos (`onFinish`, `onError`), eliminando camadas de abstraÃ§Ã£o desnecessÃ¡rias e priorizando performance e simplicidade.

## ğŸ—ï¸ Estrutura de APIs

### Routers tRPC

#### Router Principal (`chat/_router.ts`)

```typescript
export const chatRouter = {
  // SessÃµes
  listarSessions: protectedProcedure.query(),
  buscarSession: protectedProcedure.query(),
  criarSession: protectedProcedure.mutation(),
  atualizarSession: protectedProcedure.mutation(),
  excluirSession: protectedProcedure.mutation(),

  // Mensagens
  buscarMensagensTest: protectedProcedure.query(),
  enviarMensagem: protectedProcedure.mutation(),

  // Auto-criaÃ§Ã£o
  createEmptySession: protectedProcedure.mutation(),
  autoCreateSessionWithMessage: protectedProcedure.mutation(),

  // Pastas
  buscarChatFolders: protectedProcedure.query(),
  criarChatFolder: protectedProcedure.mutation(),
  atualizarChatFolder: protectedProcedure.mutation(),
  excluirChatFolder: protectedProcedure.mutation(),
  moverSession: protectedProcedure.mutation(),
} satisfies TRPCRouterRecord;
```

### Endpoint de Streaming Nativo

- **Rota**: `/api/chat/stream`
- **MÃ©todo**: POST
- **AutenticaÃ§Ã£o**: Via cookies de sessÃ£o
- **Protocolo**: Data Stream Protocol (Vercel AI SDK)
- **Sistema**: 100% `streamText()` nativo

## ğŸ—„ï¸ Modelo de Dados

### Tabela `chatSession`

```sql
CREATE TABLE chatSession (
  id VARCHAR(21) PRIMARY KEY,
  title VARCHAR(255),
  teamId VARCHAR(21) NOT NULL,
  userId VARCHAR(21) NOT NULL,
  aiModelId VARCHAR(21),
  aiAgentId VARCHAR(21),
  chatFolderId VARCHAR(21),
  createdAt TIMESTAMP DEFAULT NOW(),
  updatedAt TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),
  FOREIGN KEY (teamId) REFERENCES team(id),
  FOREIGN KEY (userId) REFERENCES user(id),
  FOREIGN KEY (aiModelId) REFERENCES aiModel(id),
  FOREIGN KEY (aiAgentId) REFERENCES aiAgent(id),
  FOREIGN KEY (chatFolderId) REFERENCES chatFolder(id)
);
```

### Tabela `chatMessage`

```sql
CREATE TABLE chatMessage (
  id VARCHAR(21) PRIMARY KEY,
  chatSessionId VARCHAR(21) NOT NULL,
  senderRole ENUM('user', 'ai', 'system') NOT NULL,
  content TEXT NOT NULL,
  status VARCHAR(50) DEFAULT 'ok',
  metadata JSON,
  createdAt TIMESTAMP DEFAULT NOW(),
  updatedAt TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),
  FOREIGN KEY (chatSessionId) REFERENCES chatSession(id) ON DELETE CASCADE
);
```

### Tabela `chatFolder`

```sql
CREATE TABLE chatFolder (
  id VARCHAR(21) PRIMARY KEY,
  name VARCHAR(255) NOT NULL,
  teamId VARCHAR(21) NOT NULL,
  userId VARCHAR(21) NOT NULL,
  isDefault BOOLEAN DEFAULT FALSE,
  createdAt TIMESTAMP DEFAULT NOW(),
  FOREIGN KEY (teamId) REFERENCES team(id),
  FOREIGN KEY (userId) REFERENCES user(id)
);
```

## ğŸ”„ Fluxo de Processamento Nativo

### 1. CriaÃ§Ã£o de SessÃ£o

```typescript
// Handler simplificado
export const criarSessionHandler = async ({ ctx, input }) => {
  const session = await chatRepository.ChatSessionRepository.create({
    title: input.title || "Nova Conversa",
    teamId: ctx.auth.user.activeTeamId,
    userId: ctx.auth.user.id,
    aiModelId: input.aiModelId,
    chatFolderId: input.chatFolderId,
  });

  return session;
};
```

### 2. Streaming com Vercel AI SDK Nativo

```typescript
// /api/chat/stream/route.ts - ImplementaÃ§Ã£o 100% Nativa
import { createAnthropic } from "@ai-sdk/anthropic";
import { createOpenAI } from "@ai-sdk/openai";
import { streamText } from "ai";

export async function POST(request: NextRequest) {
  // 1. ValidaÃ§Ã£o e preparaÃ§Ã£o
  const { chatSessionId, content } = await request.json();
  const session = await ChatService.findSessionById(chatSessionId);

  // 2. Salvar mensagem do usuÃ¡rio
  await ChatService.createMessage({
    chatSessionId: session.id,
    senderRole: "user",
    content,
    status: "ok",
  });

  // 3. Obter modelo via AI Studio Service
  const { model: vercelModel, modelName } = await getVercelModel(
    model.id,
    session.teamId,
  );

  // 4. ğŸ¯ VERCEL AI SDK NATIVO - streamText() direto
  const result = streamText({
    model: vercelModel,
    messages: formattedMessages,
    temperature: 0.7,
    maxTokens: 4000,

    // âœ… LIFECYCLE CALLBACK NATIVO - Auto-save
    onFinish: async ({ text, usage, finishReason }) => {
      console.log("âœ… [VERCEL_AI_NATIVE] Stream finished");

      // Auto-save mensagem da IA
      await ChatService.createMessage({
        chatSessionId: session.id,
        senderRole: "ai",
        content: text,
        status: "ok",
        metadata: {
          requestedModel: modelName,
          actualModelUsed: modelName,
          providerId: "vercel-ai-sdk-native",
          usage: usage || null,
          finishReason: finishReason || "stop",
          timestamp: new Date().toISOString(),
        },
      });
    },

    // âœ… LIFECYCLE CALLBACK NATIVO - Error handling
    onError: (error) => {
      console.error("ğŸ”´ [VERCEL_AI_NATIVE] Stream error:", error);
    },
  });

  // 5. Retornar response nativa
  return result.toDataStreamResponse({
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      "X-Powered-By": "Vercel-AI-SDK-Native",
    },
  });
}
```

### 3. Helper para Modelos Nativos

```typescript
// Helper function para criar modelos nativos
async function getVercelModel(modelId: string, teamId: string) {
  // Buscar configuraÃ§Ã£o do modelo via AI Studio
  const modelConfig = await AiStudioService.getModelById({
    modelId,
    teamId,
    requestingApp: chatAppId,
  });

  // Buscar token do provider
  const providerToken = await AiStudioService.getProviderToken({
    providerId: modelConfig.providerId,
    teamId,
    requestingApp: chatAppId,
  });

  // Criar provider baseado no tipo
  const providerName = modelConfig.provider.name.toLowerCase();
  const modelName = (modelConfig.config as any)?.version || modelConfig.name;

  if (providerName === "openai") {
    const openaiProvider = createOpenAI({
      apiKey: providerToken.token,
      baseURL: modelConfig.provider.baseUrl || undefined,
    });
    return {
      model: openaiProvider(modelName),
      modelName: modelName,
    };
  }

  if (providerName === "anthropic") {
    const anthropicProvider = createAnthropic({
      apiKey: providerToken.token,
      baseURL: modelConfig.provider.baseUrl || undefined,
    });
    return {
      model: anthropicProvider(modelName),
      modelName: modelName,
    };
  }

  throw new Error(
    `Provider ${modelConfig.provider.name} nÃ£o suportado. Suportados: OpenAI, Anthropic.`,
  );
}
```

## ğŸ” SeguranÃ§a e Isolamento

### Isolamento por Team

Todas as queries incluem validaÃ§Ã£o de `teamId`:

```typescript
// Repository sempre filtra por team
export const findByTeam = async (teamId: string) => {
  return db.query.chatSession.findMany({
    where: eq(chatSession.teamId, teamId),
  });
};
```

### ValidaÃ§Ã£o de PermissÃµes

```typescript
// Middleware verifica acesso
const validateSessionAccess = async (sessionId, userId, teamId) => {
  const session = await findSessionById(sessionId);

  if (!session || session.teamId !== teamId) {
    throw new TRPCError({
      code: "NOT_FOUND",
      message: "SessÃ£o nÃ£o encontrada",
    });
  }

  return session;
};
```

## ğŸ”— IntegraÃ§Ã£o com AI Studio

### Service Layer

O Chat usa `AiStudioService` para toda comunicaÃ§Ã£o:

```typescript
// Buscar modelos disponÃ­veis
const models = await AiStudioService.getAvailableModels({
  teamId: ctx.auth.user.activeTeamId,
  requestingApp: chatAppId,
});

// Buscar token do provider
const token = await AiStudioService.getProviderToken({
  providerId: model.providerId,
  teamId: ctx.auth.user.activeTeamId,
  requestingApp: chatAppId,
});
```

### Fallback de Modelo

Se a sessÃ£o nÃ£o tem modelo configurado:

1. Busca modelos disponÃ­veis do time
2. Seleciona o primeiro como padrÃ£o
3. Atualiza a sessÃ£o automaticamente

## ğŸ“Š GestÃ£o de Tokens

### CÃ¡lculo Inteligente

```typescript
// Estimar tokens de uma mensagem
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 3.5) + 10;
};

// Gerenciar limite de contexto
const manageContext = (messages, maxTokens) => {
  const maxInput = Math.floor(maxTokens * 0.7); // 70% para input

  // Preservar mensagens do sistema
  const systemMessages = messages.filter((m) => m.role === "system");

  // Truncar histÃ³rico se necessÃ¡rio
  const conversationMessages = truncateToFit(
    messages.filter((m) => m.role !== "system"),
    maxInput - estimateTokens(systemMessages),
  );

  return [...systemMessages, ...conversationMessages];
};
```

### Limites por Modelo

```typescript
const getMaxTokensForModel = (modelName: string): number => {
  const limits = {
    "gpt-4": 8192,
    "gpt-4-turbo": 128000,
    "gpt-3.5-turbo": 4096,
    "claude-3-haiku": 200000,
    "claude-3-sonnet": 200000,
    "claude-3-opus": 200000,
    "claude-3-5-sonnet": 200000,
  };

  return limits[modelName] || 4000; // Fallback conservador
};
```

## ğŸš€ Performance e OtimizaÃ§Ãµes

### Streaming Otimizado

- **Zero AbstraÃ§Ã£o**: `streamText()` direto sem wrappers
- **Lifecycle Callbacks**: `onFinish` e `onError` nativos
- **Auto-Save Integrado**: Salvamento automÃ¡tico via callback
- **Memory Efficient**: Sem buffers intermediÃ¡rios

### Caching Inteligente

```typescript
// Cache de modelos por team
const modelCache = new Map<string, any>();

const getCachedModel = async (modelId: string, teamId: string) => {
  const cacheKey = `${teamId}-${modelId}`;

  if (modelCache.has(cacheKey)) {
    return modelCache.get(cacheKey);
  }

  const model = await getVercelModel(modelId, teamId);
  modelCache.set(cacheKey, model);

  return model;
};
```

## ğŸ“ˆ Monitoramento e Logs

### Logs Estruturados

```typescript
// IdentificaÃ§Ã£o clara do sistema
console.log("ğŸš€ [VERCEL_AI_NATIVE] Iniciando streaming");
console.log("âœ… [VERCEL_AI_NATIVE] Stream finished");
console.log("ğŸ”´ [VERCEL_AI_NATIVE] Stream error");
```

### MÃ©tricas de Performance

```typescript
// Tracking de performance
const startTime = performance.now();

// ... processamento ...

const duration = performance.now() - startTime;
console.log(`â±ï¸ [PERFORMANCE] Stream completed in ${duration}ms`);
```

## ğŸ”§ Troubleshooting

### Problemas Comuns

1. **Modelo nÃ£o encontrado**

   - Verificar se modelo estÃ¡ habilitado no AI Studio
   - Confirmar permissÃµes do team

2. **Token invÃ¡lido**

   - Validar configuraÃ§Ã£o do provider no AI Studio
   - Verificar se token nÃ£o expirou

3. **Streaming interrompido**
   - Verificar logs do `onError` callback
   - Confirmar conectividade com provider

### Debug Mode

```typescript
// Ativar logs detalhados
process.env.DEBUG_CHAT = "true";

// Logs aparecem como:
// ğŸ” [DEBUG] Model config: {...}
// ğŸ” [DEBUG] Messages: {...}
// ğŸ” [DEBUG] Stream result: {...}
```

---

## ğŸ“š ReferÃªncias

- **[Vercel AI SDK](https://sdk.vercel.ai/)** - DocumentaÃ§Ã£o oficial
- **[AI Studio Service](../../ai-studio/backend-architecture.md)** - IntegraÃ§Ã£o com modelos
- **[Chat Frontend](./frontend-architecture.md)** - Arquitetura do frontend
- **[Session Management](./session-management.md)** - GestÃ£o de sessÃµes

---

**âœ… Sistema 100% Nativo Operacional**

**ğŸ¯ BenefÃ­cios AlcanÃ§ados:**

- âœ… Zero abstraÃ§Ãµes desnecessÃ¡rias
- âœ… Performance mÃ¡xima com `streamText()` direto
- âœ… Auto-save integrado via lifecycle callbacks
- âœ… Error handling robusto com `onError`
- âœ… Compatibilidade total com Vercel AI SDK
- âœ… CÃ³digo 62% mais limpo que sistema anterior
