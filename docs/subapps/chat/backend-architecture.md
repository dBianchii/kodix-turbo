# Backend Architecture - Chat SubApp

## ‚öôÔ∏è Vis√£o Geral

O backend do Chat √© constru√≠do com tRPC para APIs type-safe e um endpoint HTTP dedicado para streaming. A arquitetura utiliza um **sistema h√≠brido** com Vercel AI SDK como engine principal e sistema legacy como fallback, priorizando performance e integra√ß√£o segura com o AI Studio.

## üèóÔ∏è Estrutura de APIs

### Routers tRPC

#### Router Principal (`chat/_router.ts`)

```typescript
export const chatRouter = {
  // Sess√µes
  buscarSessoes: protectedProcedure.query(),
  criarSessao: protectedProcedure.mutation(),
  atualizarSessao: protectedProcedure.mutation(),
  deletarSessao: protectedProcedure.mutation(),

  // Mensagens
  buscarMensagens: protectedProcedure.query(),
  enviarMensagem: protectedProcedure.mutation(),

  // Auto-cria√ß√£o
  autoCreateSessionWithMessage: protectedProcedure.mutation(),
} satisfies TRPCRouterRecord;
```

### Endpoint de Streaming H√≠brido

- **Rota**: `/api/chat/stream`
- **M√©todo**: POST
- **Autentica√ß√£o**: Via cookies de sess√£o
- **Protocolo**: Server-Sent Events (SSE)
- **Sistema**: H√≠brido (Vercel AI SDK + Legacy Fallback)

## üóÑÔ∏è Modelo de Dados

### Tabela `chatSession`

```sql
CREATE TABLE chatSession (
  id VARCHAR(21) PRIMARY KEY,
  title VARCHAR(255),
  teamId VARCHAR(21) NOT NULL,
  userId VARCHAR(21) NOT NULL,
  aiModelId VARCHAR(21),
  aiAgentId VARCHAR(21),
  createdAt TIMESTAMP DEFAULT NOW(),
  updatedAt TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),
  FOREIGN KEY (teamId) REFERENCES team(id),
  FOREIGN KEY (userId) REFERENCES user(id),
  FOREIGN KEY (aiModelId) REFERENCES aiModel(id),
  FOREIGN KEY (aiAgentId) REFERENCES aiAgent(id)
);
```

### Tabela `chatMessage`

```sql
CREATE TABLE chatMessage (
  id VARCHAR(21) PRIMARY KEY,
  chatSessionId VARCHAR(21) NOT NULL,
  senderRole ENUM('user', 'ai', 'system') NOT NULL,
  content TEXT NOT NULL,
  status VARCHAR(50) DEFAULT 'ok',
  metadata JSON,
  createdAt TIMESTAMP DEFAULT NOW(),
  updatedAt TIMESTAMP DEFAULT NOW() ON UPDATE NOW(),
  FOREIGN KEY (chatSessionId) REFERENCES chatSession(id) ON DELETE CASCADE
);
```

## üîÑ Fluxo de Processamento H√≠brido

### 1. Cria√ß√£o de Sess√£o

```typescript
// Handler simplificado
export const criarSessaoHandler = async ({ ctx, input }) => {
  const session = await chatRepository.ChatSessionRepository.create({
    title: input.title || "Nova Conversa",
    teamId: ctx.auth.user.activeTeamId,
    userId: ctx.auth.user.id,
    aiModelId: input.modelId,
  });

  return session;
};
```

### 2. Envio de Mensagem

O fluxo de envio envolve m√∫ltiplas etapas:

1. **Valida√ß√£o**: Verifica sess√£o e permiss√µes
2. **Persist√™ncia**: Salva mensagem do usu√°rio
3. **Decis√£o de Sistema**: Vercel AI SDK ou Legacy
4. **Streaming**: Inicia resposta da IA
5. **Finaliza√ß√£o**: Salva resposta completa

### 3. Streaming H√≠brido de Resposta

```typescript
// Endpoint de streaming h√≠brido
export async function POST(request: NextRequest) {
  // 1. Validar sess√£o e autentica√ß√£o
  const { chatSessionId, content } = await request.json();
  const session = await ChatService.findSessionById(chatSessionId);

  // 2. DECIS√ÉO DE SISTEMA: Vercel AI SDK vs Legacy
  if (FEATURE_FLAGS.VERCEL_AI_ADAPTER) {
    console.log("üöÄ [MIGRATION] Usando Vercel AI SDK via adapter");

    try {
      // 3A. VERCEL AI SDK (Sistema Principal)
      const adapter = new VercelAIAdapter();
      const result = await adapter.streamResponse({
        chatSessionId: session.id,
        content,
        modelId: session.aiModelId,
        teamId: session.teamId,
        messages: formattedMessages,
        temperature: 0.7,
        maxTokens: 4000,
      });

      return new NextResponse(result.stream, {
        headers: {
          "Content-Type": "text/plain; charset=utf-8",
          "Cache-Control": "no-cache",
          "X-Powered-By": "Vercel-AI-SDK", // Identifica√ß√£o
        },
      });
    } catch (error) {
      console.error("üî¥ [MIGRATION] Erro no Vercel AI SDK, fallback:", error);
      // Continua para sistema legacy automaticamente
    }
  }

  // 3B. SISTEMA LEGACY (Fallback ou quando flag desabilitada)
  console.log("üîÑ [LEGACY] Usando sistema atual de streaming");

  // Buscar modelo e configura√ß√µes via AI Studio
  const model = await AiStudioService.getModelById({
    modelId: session.aiModelId,
    teamId: session.teamId,
    requestingApp: chatAppId,
  });

  const token = await AiStudioService.getProviderToken({
    providerId: model.providerId,
    teamId: session.teamId,
    requestingApp: chatAppId,
  });

  // Configurar streaming direto com provider
  const stream = await createLegacyStreamingResponse({
    model,
    messages: formattedMessages,
    token: token.token,
  });

  return new Response(stream, {
    headers: {
      "Content-Type": "text/plain; charset=utf-8",
      "Cache-Control": "no-cache",
      // Sem header X-Powered-By (identifica sistema legacy)
    },
  });
}
```

## üîß Sistema H√≠brido: Vercel AI SDK + Legacy

### VercelAIAdapter (Sistema Principal)

```typescript
// packages/api/src/internal/adapters/vercel-ai-adapter.ts
export class VercelAIAdapter {
  async streamResponse(params: ChatStreamParams): Promise<ChatStreamResponse> {
    // 1. Buscar modelo via AI Studio
    const model = await this.getVercelModel(params.modelId, params.teamId);

    // 2. Executar streaming com Vercel AI SDK
    const result = await streamText({
      model,
      messages: this.adaptInputParams(params).messages,
      temperature: params.temperature || 0.7,
      maxTokens: params.maxTokens || 4000,
    });

    // 3. Adaptar resposta para formato atual
    return this.adaptResponse(result);
  }

  private async getVercelModel(modelId: string, teamId: string) {
    const modelConfig = await AiStudioService.getModelById({
      modelId,
      teamId,
      requestingApp: chatAppId,
    });

    const providerToken = await AiStudioService.getProviderToken({
      providerId: modelConfig.providerId,
      teamId,
      requestingApp: chatAppId,
    });

    // Suporte a m√∫ltiplos providers via Vercel AI SDK
    switch (modelConfig.provider.name.toLowerCase()) {
      case "openai":
        return openai(modelConfig.name, {
          apiKey: providerToken.token,
          baseURL: modelConfig.provider.baseUrl,
        });

      case "anthropic":
        return anthropic(modelConfig.name, {
          apiKey: providerToken.token,
        });

      default:
        throw new Error(`Provider ${modelConfig.provider.name} n√£o suportado`);
    }
  }
}
```

### Sistema Legacy (Fallback)

```typescript
// Sistema legacy mantido como fallback
const createLegacyStreamingResponse = async ({ model, messages, token }) => {
  const apiUrl = `${model.provider.baseUrl}/chat/completions`;

  const response = await fetch(apiUrl, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: model.name,
      messages: messages,
      stream: true,
      temperature: 0.7,
      max_tokens: 4000,
    }),
  });

  return response.body; // Stream direto do provider
};
```

## üîê Seguran√ßa e Isolamento

### Isolamento por Team

Todas as queries incluem valida√ß√£o de `teamId`:

```typescript
// Repository sempre filtra por team
export const findByTeam = async (teamId: string) => {
  return db.query.chatSession.findMany({
    where: eq(chatSession.teamId, teamId),
  });
};
```

### Valida√ß√£o de Permiss√µes

```typescript
// Middleware verifica acesso
const validateSessionAccess = async (sessionId, userId, teamId) => {
  const session = await findSessionById(sessionId);

  if (!session || session.teamId !== teamId) {
    throw new TRPCError({
      code: "NOT_FOUND",
      message: "Sess√£o n√£o encontrada",
    });
  }

  return session;
};
```

## üîó Integra√ß√£o com AI Studio

### Service Layer

O Chat usa `AiStudioService` para toda comunica√ß√£o:

```typescript
// Buscar modelos dispon√≠veis
const models = await AiStudioService.getAvailableModels({
  teamId: ctx.auth.user.activeTeamId,
  requestingApp: chatAppId,
});

// Buscar token do provider
const token = await AiStudioService.getProviderToken({
  providerId: model.providerId,
  teamId: ctx.auth.user.activeTeamId,
  requestingApp: chatAppId,
});
```

### Fallback de Modelo

Se a sess√£o n√£o tem modelo configurado:

1. Busca modelos dispon√≠veis do time
2. Seleciona o primeiro como padr√£o
3. Atualiza a sess√£o automaticamente

## üìä Gest√£o de Tokens

### C√°lculo Inteligente

```typescript
// Estimar tokens de uma mensagem
const estimateTokens = (text: string): number => {
  return Math.ceil(text.length / 3.5) + 10;
};

// Gerenciar limite de contexto
const manageContext = (messages, maxTokens) => {
  const maxInput = Math.floor(maxTokens * 0.7); // 70% para input

  // Preservar mensagens do sistema
  const systemMessages = messages.filter((m) => m.role === "system");

  // Truncar hist√≥rico se necess√°rio
  const conversationMessages = truncateToFit(
    messages.filter((m) => m.role !== "system"),
    maxInput - estimateTokens(systemMessages),
  );

  return [...systemMessages, ...conversationMessages];
};
```

### Limites por Modelo

O sistema obt√©m limites espec√≠ficos do AI Studio:

- Limites configurados por modelo no AI Studio
- Fallback para limites padr√£o se n√£o especificado
- Consulte [Model Configuration](../ai-studio/model-configuration.md) para detalhes

## üåç Internacionaliza√ß√£o

### System Prompts Multil√≠ngues

```typescript
// Detectar idioma do usu√°rio
const userLocale = detectUserLocale(request);

// Aplicar prompt no idioma correto
const systemPrompt =
  userLocale === "pt-BR"
    ? "Voc√™ √© um assistente √∫til e responde sempre em portugu√™s brasileiro."
    : "You are a helpful assistant and always respond in English.";
```

### Detec√ß√£o de Idioma

1. Cookie `NEXT_LOCALE`
2. Pathname da URL
3. Header `Accept-Language`
4. Fallback para `pt-BR`

## üöÄ Performance

### Otimiza√ß√µes Implementadas

- **Streaming H√≠brido**: Vercel AI SDK (principal) + Legacy (fallback)
- **Pagina√ß√£o**: Mensagens carregadas sob demanda
- **Cache**: Modelos dispon√≠veis s√£o cacheados
- **√çndices**: Queries otimizadas com √≠ndices apropriados
- **Fallback Transparente**: Zero downtime em caso de problemas

### M√©tricas Monitoradas

- Tempo de resposta do primeiro token
- Taxa de sucesso/erro das APIs
- Taxa de fallback (Vercel AI SDK ‚Üí Legacy)
- Uso de tokens por sess√£o
- Lat√™ncia do streaming por sistema

## üîß Tratamento de Erros

### Erros Espec√≠ficos de Provider

```typescript
// Token inv√°lido
if (response.status === 401) {
  throw new Error("Token inv√°lido. Verifique configura√ß√£o no AI Studio");
}

// Limite excedido
if (response.status === 429) {
  throw new Error("Limite de uso excedido. Verifique sua conta do provider");
}

// Modelo n√£o encontrado
if (response.status === 404) {
  throw new Error(`Modelo n√£o encontrado. Configure no AI Studio`);
}
```

### Recovery Strategies

1. **Vercel AI SDK falha**: Fallback autom√°tico para sistema legacy
2. **Modelo n√£o dispon√≠vel**: Fallback para modelo padr√£o via AI Studio
3. **Token expirado**: Redirecionar para configura√ß√£o no AI Studio
4. **Limite excedido**: Sugerir truncar contexto ou trocar modelo

## üìù Logs e Auditoria

### Logs de Sistema H√≠brido

```typescript
// Vercel AI SDK ativo
console.log("üöÄ [MIGRATION] Usando Vercel AI SDK via adapter");

// Sistema legacy (fallback)
console.log("üîÑ [LEGACY] Usando sistema atual de streaming");

// Fallback autom√°tico
console.error("üî¥ [MIGRATION] Erro no Vercel AI SDK, fallback:", error);
```

### Metadata de Mensagens

Cada mensagem salva metadata relevante:

```json
{
  "requestedModel": "gpt-4",
  "actualModelUsed": "gpt-4",
  "providerId": "vercel-ai-sdk", // ou provider espec√≠fico
  "providerName": "Vercel AI SDK", // ou "OpenAI", "Anthropic"
  "migration": "subetapa-6", // Identifica√ß√£o do sistema h√≠brido
  "usage": {
    "promptTokens": 150,
    "completionTokens": 200,
    "totalTokens": 350
  },
  "timestamp": "2024-01-01T00:00:00Z"
}
```

## üîÑ Pr√≥ximas Melhorias

### Planejadas

- [ ] Remo√ß√£o gradual do sistema legacy
- [ ] Suporte a m√∫ltiplos agentes via Vercel AI SDK
- [ ] Tools/Functions integration
- [ ] Structured output capabilities
- [ ] M√©tricas detalhadas de fallback
- [ ] Cache de respostas comuns
- [ ] Compress√£o de hist√≥rico
- [ ] Webhooks para integra√ß√µes

---

**üéâ Backend h√≠brido robusto: Vercel AI SDK como principal + Sistema legacy como fallback confi√°vel!**
