# Plano de Migra√ß√£o: AI Studio como Motor de Execu√ß√£o de IA

**Data:** 2025-06-28  
**Autor:** KodixAgent  
**Status:** üü° Proposta
**Escopo:** AI Studio & Chat SubApps - Backend
**Tipo:** Refatora√ß√£o Arquitetural
**Documento Pai:** `docs/subapps/ai-studio/ai-studio-architecture.md`

---

## 1. Resumo Executivo

Este plano descreve a evolu√ß√£o arquitetural para posicionar o **AI Studio como o motor central de execu√ß√£o de IA** para toda a plataforma Kodix. Atualmente, o `Chat Stream API` √© respons√°vel por chamar o Vercel AI SDK. A proposta √© mover essa responsabilidade para o `AiStudioService`.

Com esta mudan√ßa, o Chat (e qualquer outro SubApp) se tornar√° completamente agn√≥stico √† implementa√ß√£o da IA. Ele apenas solicitar√° um stream de resposta, e o `AiStudioService` cuidar√° de toda a intera√ß√£o com o SDK, desde a constru√ß√£o do provedor at√© o streaming da resposta e os callbacks de conclus√£o.

### Objetivos

- ‚úÖ Centralizar 100% da l√≥gica de intera√ß√£o com o Vercel AI SDK dentro do `AiStudioService`.
- ‚úÖ Refatorar o `Chat Stream API` para atuar como um proxy leve, delegando a execu√ß√£o ao `AiStudioService`.
- ‚úÖ Abstrair completamente a tecnologia de IA dos SubApps consumidores.
- ‚úÖ Aumentar a manutenibilidade, a flexibilidade e a consist√™ncia do sistema de IA.

---

## 2. An√°lise Arquitetural: Antes e Depois

### Arquitetura Atual

O Chat conhece e interage diretamente com o Vercel AI SDK.

```mermaid
graph TD
    subgraph "Dom√≠nio do Chat"
        A[Chat Stream API] --> |"2. Usa config para chamar"| C[Vercel AI SDK]
    end
    subgraph "Dom√≠nio do AI Studio"
        B[AiStudioService]
    end

    A -->|"1. Pede config (prompt, modelo)"| B
    B -->|Retorna config| A
```

### Arquitetura Proposta

O Chat apenas pede um stream de resposta. O AI Studio gerencia toda a intera√ß√£o com o Vercel AI SDK.

```mermaid
graph TD
    subgraph "Dom√≠nio do Chat"
        A[Chat Stream API]
    end
    subgraph "Dom√≠nio do AI Studio (Motor de IA)"
        B[AiStudioService]
        C[Vercel AI SDK]
    end

    A -->|"Me d√™ um stream de resposta<br/>para esta conversa"| B
    B -->|"Ok, eu sei como fazer isso"| C
    C -->|Retorna o stream| B
    B -->|Encaminha o stream| A
```

---

## 3. Plano de Implementa√ß√£o Detalhado

### Passo 1: Expandir o `AiStudioService`

O `AiStudioService` ganhar√° um novo m√©todo que encapsula a l√≥gica do Vercel AI SDK.

**Exemplo de Implementa√ß√£o (`packages/api/src/internal/services/ai-studio.service.ts`):**

```typescript
// ... imports
import type { CoreMessage } from "ai";
import { createAnthropic } from "@ai-sdk/anthropic";
import { createOpenAI } from "@ai-sdk/openai";
import { streamText } from "ai";

import { ChatService } from "./chat.service"; // Para o callback onFinish

export class AiStudioService extends BaseService {
  // ... m√©todos existentes como getModelById, getProviderToken...

  /**
   * Orquestra e executa o streaming de uma resposta de IA.
   * Este m√©todo √© o novo "motor de IA" da plataforma.
   */
  static async streamChatResponse(context: {
    teamId: string;
    userId: string;
    chatSessionId: string;
    messages: CoreMessage[];
    requestingApp: KodixAppId;
  }) {
    const { teamId, userId, chatSessionId, messages, requestingApp } = context;

    this.validateTeamAccess(teamId);
    this.logAccess("streamChatResponse", { teamId, requestingApp, userId });

    // 1. Obter informa√ß√µes do modelo e token (l√≥gica existente)
    const session = await ChatService.findSessionById(chatSessionId); // Assume-se valida√ß√£o de acesso
    const modelInfo = await this.getModelById({
      modelId: session.aiModelId,
      teamId,
      requestingApp,
    });
    const token = await this.getProviderToken({
      providerId: modelInfo.providerId,
      teamId,
      requestingApp,
    });

    // 2. Criar o provedor de modelo nativo do Vercel AI SDK
    const vercelProvider = this.createVercelProvider(modelInfo, token);

    // 3. Executar o streaming com callbacks
    const result = streamText({
      model: vercelProvider,
      messages: messages,
      // Passar outros par√¢metros como temperature, maxTokens, etc.

      onFinish: async ({ text, usage, finishReason }) => {
        // A l√≥gica de salvar a mensagem da IA agora vive aqui
        await ChatService.createMessage({
          chatSessionId: chatSessionId,
          senderRole: "ai",
          content: text,
          status: "ok",
          metadata: {
            usage,
            finishReason,
            actualModelUsed: modelInfo.name,
            timestamp: new Date().toISOString(),
          },
        });
      },
      onError: (error) => {
        console.error("üî¥ [AI_STUDIO_SERVICE_STREAM] Erro no stream:", error);
        // Adicionar l√≥gica de notifica√ß√£o ou logging de erro robusto aqui
      },
    });

    // 4. Retornar o stream para ser encaminhado pela API
    return result.toDataStreamResponse();
  }

  private static createVercelProvider(modelInfo, token) {
    switch (modelInfo.provider.name.toLowerCase()) {
      case "openai":
        return createOpenAI({ apiKey: token });
      case "anthropic":
        return createAnthropic({ apiKey: token });
      // Adicionar outros provedores aqui
      default:
        throw new TRPCError({
          code: "BAD_REQUEST",
          message: `Provedor ${modelInfo.provider.name} n√£o suportado.`,
        });
    }
  }
}
```

### Passo 2: Simplificar o `Chat Stream API`

O endpoint do Chat ser√° refatorado para ser um simples proxy.

**Exemplo de Refatora√ß√£o (`apps/kdx/src/app/api/chat/stream/route.ts`):**

```typescript
// ‚ùå Remover imports diretos do Vercel AI SDK: createAnthropic, createOpenAI, streamText
import { type NextRequest } from "next/server";
import { type CoreMessage } from "ai";

import { AiStudioService } from "@kdx/api/internal/services/ai-studio.service";
import { ChatService } from "@kdx/api/internal/services/chat.service";
import { auth } from "@kdx/auth";
import { chatAppId } from "@kdx/shared";

export async function POST(request: NextRequest) {
  const { messages, chatSessionId } = (await request.json()) as {
    messages: CoreMessage[];
    chatSessionId: string;
  };

  const session = await auth();
  if (!session?.user) return new Response("Unauthorized", { status: 401 });

  const { id: userId, activeTeamId: teamId } = session.user;

  // 1. Salvar a mensagem do usu√°rio (esta responsabilidade pode permanecer aqui)
  const lastMessage = messages[messages.length - 1];
  if (lastMessage?.role === "user") {
    await ChatService.createMessage({
      chatSessionId: chatSessionId,
      senderRole: "user",
      content: lastMessage.content,
      status: "ok",
    });
  }

  // 2. Obter o `systemPrompt` atrav√©s do AiStudioService (conforme plano anterior)
  const systemPrompt = await AiStudioService.getSystemPromptForChat({
    userId,
    teamId,
    requestingApp: chatAppId,
  });

  const finalMessages = [
    { role: "system", content: systemPrompt },
    ...messages,
  ];

  // 3. DELEGAR a execu√ß√£o do stream para o AiStudioService
  return AiStudioService.streamChatResponse({
    teamId,
    userId,
    chatSessionId,
    messages: finalMessages,
    requestingApp: chatAppId,
  });
}
```

---

## 4. Checklist de Implementa√ß√£o

### Backend (2-3 dias)

- [ ] **`AiStudioService`**:

  - [ ] Criar o novo m√©todo `streamChatResponse`.
  - [ ] Implementar a l√≥gica de cria√ß√£o de provedor (`createVercelProvider`).
  - [ ] Mover a chamada `streamText` e seus callbacks para dentro do novo m√©todo.
  - [ ] Garantir que o m√©todo retorna um `ReadableStream` atrav√©s de `toDataStreamResponse()`.
  - [ ] Adicionar testes unit√°rios para o novo m√©todo, mockando os servi√ßos dependentes.

- [ ] **`Chat Stream API` (`/api/chat/stream/route.ts`)**:
  - [ ] Remover todas as importa√ß√µes e l√≥gica direta do Vercel AI SDK.
  - [ ] Refatorar o handler `POST` para chamar `AiStudioService.streamChatResponse`.
  - [ ] Garantir que o `systemPrompt` √© obtido e adicionado √†s mensagens.
  - [ ] Assegurar que o stream retornado pelo servi√ßo seja corretamente encaminhado como resposta da API.

### Documenta√ß√£o (4 horas)

- [ ] Atualizar `docs/subapps/chat/chat-architecture.md` para refletir o novo fluxo simplificado.
- [ ] Atualizar `docs/subapps/ai-studio/ai-studio-architecture.md` para documentar a nova e importante responsabilidade do `AiStudioService`.
- [ ] Marcar este plano como conclu√≠do.

### Teste E2E (1 dia)

- [ ] Testar o fluxo de chat completo para m√∫ltiplos provedores (OpenAI, Anthropic).
- [ ] Verificar se as mensagens do usu√°rio e da IA s√£o salvas corretamente no banco de dados (`onFinish` callback).
- [ ] Inspecionar os logs para confirmar que os erros de stream s√£o capturados e registrados pelo `AiStudioService`.
- [ ] Validar que as instru√ß√µes da plataforma, time e usu√°rio ainda s√£o corretamente aplicadas ao `systemPrompt`.

---

## 5. Benef√≠cios Esperados

- **Abstra√ß√£o Total:** O Chat e outros SubApps n√£o precisam saber qual tecnologia de IA est√° sendo usada.
- **Ponto √önico de Manuten√ß√£o:** Atualiza√ß√µes do Vercel AI SDK ou de provedores s√£o feitas em um √∫nico lugar.
- **Flexibilidade Futura:** Torna trivial adicionar novos "motores de IA" (como LangChain) ou trocar os existentes.
- **Consist√™ncia Garantida:** Tratamento de erros, callbacks e formata√ß√£o de respostas s√£o padronizados para toda a plataforma.
