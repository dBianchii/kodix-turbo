import { TRPCError } from "@trpc/server";

import type { AutoCreateSessionWithMessageInput } from "@kdx/validators/trpc/app";
import { appRepository, chatRepository } from "@kdx/db/repositories";
import { chatAppId } from "@kdx/shared";

import type { TProtectedProcedureContext } from "../../../procedures";
import { AiStudioService } from "../../../../internal/services/ai-studio.service";

// Helper para buscar modelo preferido seguindo hierarquia usando Service Layer
async function getPreferredModelHelper(
  teamId: string,
  userId: string,
  requestingApp: typeof chatAppId,
): Promise<{
  source: "user_config" | "ai_studio_default" | "first_available";
  modelId: string;
  model: any;
  config?: any;
  teamConfig?: any;
}> {
  // ‚úÖ 1¬™ Prioridade: Verificar preferredModelId nas configura√ß√µes de USU√ÅRIO
  try {
    console.log("üîç [PREFERRED_MODEL] Buscando User Config para:", {
      teamId,
      userId,
    });

    const userConfigs = await appRepository.findUserAppTeamConfigs({
      appId: chatAppId,
      teamIds: [teamId],
      userIds: [userId],
    });

    console.log("üìä [PREFERRED_MODEL] User configs encontrados:", {
      count: userConfigs.length,
      configs: userConfigs.map((c) => ({
        id: c.id,
        config: c.config,
        hasConfig: !!c.config,
        hasPersonalSettings: !!(c.config as any)?.personalSettings,
        preferredModelId: (c.config as any)?.personalSettings?.preferredModelId,
      })),
    });

    const userConfig = userConfigs[0]; // S√≥ haver√° um config por usu√°rio/app/team
    const preferredModelId = userConfig
      ? (userConfig.config as any)?.personalSettings?.preferredModelId
      : null;

    console.log("üéØ [PREFERRED_MODEL] Extracted preferredModelId:", {
      preferredModelId,
      hasUserConfig: !!userConfig,
      fullConfig: userConfig?.config,
    });

    if (preferredModelId) {
      console.log(
        "‚úÖ [PREFERRED_MODEL] Encontrado preferredModelId no User Config:",
        preferredModelId,
      );

      try {
        const model = await AiStudioService.getModelById({
          modelId: preferredModelId,
          teamId,
          requestingApp,
        });

        if (model) {
          console.log(
            "‚úÖ [PREFERRED_MODEL] Modelo encontrado (User Config):",
            model.name,
          );
          return {
            source: "user_config",
            modelId: model.id,
            model,
            config: userConfig?.config,
          };
        }
      } catch (error) {
        console.log(
          "‚ö†Ô∏è [PREFERRED_MODEL] preferredModelId do User Config inv√°lido, continuando para AI Studio",
        );
      }
    } else {
      console.log(
        "‚ùå [PREFERRED_MODEL] Nenhum preferredModelId encontrado no User Config",
      );
    }
  } catch (error) {
    console.log(
      "‚ö†Ô∏è [PREFERRED_MODEL] Erro ao buscar User Config, continuando para AI Studio:",
      error,
    );
  }

  // ‚úÖ 2¬™ Prioridade: Buscar modelo padr√£o no AI Studio via Service Layer
  try {
    const defaultModelConfig = await AiStudioService.getDefaultModel({
      teamId,
      requestingApp,
    });

    if (defaultModelConfig.model) {
      console.log(
        "‚úÖ [PREFERRED_MODEL] Modelo padr√£o do AI Studio encontrado:",
        defaultModelConfig.model.name,
      );
      return {
        source: "ai_studio_default",
        modelId: defaultModelConfig.model.id,
        model: defaultModelConfig.model,
        teamConfig: defaultModelConfig,
      };
    }
  } catch (error) {
    console.log(
      "‚ö†Ô∏è [PREFERRED_MODEL] Erro ao buscar modelo padr√£o do AI Studio:",
      error,
    );
  }

  // ‚úÖ 3¬™ Prioridade: Buscar primeiro modelo ativo dispon√≠vel via Service Layer
  try {
    const availableModels = await AiStudioService.getAvailableModels({
      teamId,
      requestingApp,
    });

    const firstActiveModel = (availableModels || []).find(
      (m: any) => m.teamConfig?.enabled,
    );

    if (firstActiveModel) {
      console.log(
        "üîÑ [PREFERRED_MODEL] Usando primeiro modelo ativo como fallback:",
        firstActiveModel.name,
      );
      return {
        source: "first_available",
        modelId: firstActiveModel.id,
        model: firstActiveModel,
        teamConfig: firstActiveModel.teamConfig,
      };
    }
  } catch (error) {
    console.log(
      "‚ö†Ô∏è [PREFERRED_MODEL] Erro ao buscar modelos dispon√≠veis:",
      error,
    );
  }

  throw new TRPCError({
    code: "NOT_FOUND",
    message: "Nenhum modelo de IA dispon√≠vel. Configure modelos no AI Studio.",
  });
}

export async function autoCreateSessionWithMessageHandler({
  input,
  ctx,
}: {
  input: AutoCreateSessionWithMessageInput;
  ctx: TProtectedProcedureContext;
}) {
  const teamId = ctx.auth.user.activeTeamId;
  const userId = ctx.auth.user.id;

  try {
    console.log(
      "üöÄ [AUTO_CREATE] Iniciando auto-cria√ß√£o de sess√£o para team:",
      teamId,
    );

    // 1. Buscar modelo preferido
    let preferredModel;
    let aiModelId: string;

    try {
      // Chamar getPreferredModel internamente
      const preferredModelResult = await getPreferredModelHelper(
        teamId,
        userId,
        chatAppId,
      );

      preferredModel = preferredModelResult.model;
      aiModelId = preferredModelResult.modelId;

      console.log(
        "‚úÖ [AUTO_CREATE] Modelo preferido encontrado:",
        preferredModel.name,
      );
    } catch (error) {
      console.error("‚ùå [AUTO_CREATE] Erro ao buscar modelo preferido:", error);
      throw new TRPCError({
        code: "PRECONDITION_FAILED",
        message:
          "Nenhum modelo de IA dispon√≠vel. Configure modelos no AI Studio.",
      });
    }

    // 2. Gerar t√≠tulo automaticamente (se habilitado)
    let title = "Nova Conversa";

    if (input.generateTitle) {
      try {
        console.log("ü§ñ [AUTO_CREATE] Gerando t√≠tulo autom√°tico...");

        // Buscar token do provider via HTTP
        const providerToken = await AiStudioService.getProviderToken({
          providerId: preferredModel.providerId,
          teamId,
          requestingApp: chatAppId,
        });

        if (providerToken.token) {
          // Configurar API baseada no provider
          const baseUrl =
            preferredModel.provider?.baseUrl || "https://api.openai.com/v1";
          const apiUrl = `${baseUrl}/chat/completions`;

          // Usar configura√ß√µes do modelo
          const modelConfig = (preferredModel.config || {}) as {
            version?: string;
            maxTokens?: number;
            temperature?: number;
          };
          const modelName = modelConfig.version || preferredModel.name;

          // Prompt para gerar t√≠tulo
          const titlePrompt = [
            {
              role: "system",
              content:
                "Voc√™ √© um assistente especializado em criar t√≠tulos concisos. Crie um t√≠tulo curto (m√°ximo 50 caracteres) que capture a ess√™ncia da mensagem do usu√°rio. Responda apenas com o t√≠tulo, sem aspas ou formata√ß√£o adicional.",
            },
            {
              role: "user",
              content: `Crie um t√≠tulo para esta conversa: "${input.firstMessage}"`,
            },
          ];

          const response = await fetch(apiUrl, {
            method: "POST",
            headers: {
              Authorization: `Bearer ${providerToken.token}`,
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              model: modelName,
              messages: titlePrompt,
              max_tokens: 20,
              temperature: 0.7,
            }),
          });

          if (response.ok) {
            const aiResponse = (await response.json()) as any;
            const generatedTitle =
              aiResponse.choices?.[0]?.message?.content?.trim();

            if (generatedTitle && generatedTitle.length <= 50) {
              title = generatedTitle;
              console.log("‚úÖ [AUTO_CREATE] T√≠tulo gerado:", title);
            }
          }
        }
      } catch (error) {
        console.log(
          "‚ö†Ô∏è [AUTO_CREATE] Erro ao gerar t√≠tulo, usando padr√£o:",
          error,
        );
        // Fallback: usar primeiros 50 caracteres da mensagem
        title = input.firstMessage.slice(0, 50);
        if (input.firstMessage.length > 50) {
          title += "...";
        }
      }
    }

    // 3. Criar sess√£o
    const session = await chatRepository.ChatSessionRepository.create({
      title,
      aiModelId,
      teamId,
      userId,
    });

    if (!session?.id) {
      throw new TRPCError({
        code: "INTERNAL_SERVER_ERROR",
        message: "Erro ao criar sess√£o de chat",
      });
    }

    console.log("‚úÖ [AUTO_CREATE] Sess√£o criada:", session.id);

    // 4. Criar primeira mensagem do usu√°rio
    const userMessage = await chatRepository.ChatMessageRepository.create({
      chatSessionId: session.id,
      senderRole: "user",
      content: input.firstMessage,
      status: "ok",
    });

    console.log("‚úÖ [AUTO_CREATE] Primeira mensagem criada");

    // 5. Se useAgent, processar resposta da IA
    let aiMessage = null;
    if (input.useAgent) {
      try {
        // Buscar token do provider via HTTP
        const providerToken = await AiStudioService.getProviderToken({
          providerId: preferredModel.providerId,
          teamId,
          requestingApp: chatAppId,
        });

        if (!providerToken.token) {
          throw new Error(
            `Token n√£o configurado para o provider ${preferredModel.provider?.name || "provider"}`,
          );
        }

        // Configurar API baseada no provider
        const baseUrl =
          preferredModel.provider?.baseUrl || "https://api.openai.com/v1";
        const apiUrl = `${baseUrl}/chat/completions`;

        // Usar configura√ß√µes do modelo
        const modelConfig = (preferredModel.config || {}) as {
          version?: string;
          maxTokens?: number;
          temperature?: number;
        };
        const modelName = modelConfig.version || preferredModel.name;
        const maxTokens = modelConfig.maxTokens || 500;
        const temperature = modelConfig.temperature || 0.7;

        // Fazer chamada para IA
        const response = await fetch(apiUrl, {
          method: "POST",
          headers: {
            Authorization: `Bearer ${providerToken.token}`,
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            model: modelName,
            messages: [
              {
                role: "user",
                content: input.firstMessage,
              },
            ],
            max_tokens: maxTokens,
            temperature: temperature,
          }),
        });

        if (!response.ok) {
          const errorText = await response.text();
          throw new Error(
            `Erro na API do ${preferredModel.provider?.name || "provider"}: ${response.status} - ${errorText}`,
          );
        }

        const aiResponse = (await response.json()) as any;
        const aiContent =
          aiResponse.choices?.[0]?.message?.content ||
          "Desculpe, n√£o consegui gerar uma resposta.";

        // ‚úÖ Extrair modelo retornado pela API
        const actualModelUsed = aiResponse.model || modelName;

        // ‚úÖ Criar metadata com informa√ß√µes do modelo
        const messageMetadata = {
          requestedModel: modelName,
          actualModelUsed: actualModelUsed,
          providerId: preferredModel.providerId,
          providerName: preferredModel.provider?.name,
          usage: aiResponse.usage || null,
          timestamp: new Date().toISOString(),
        };

        console.log(
          `üîç [AUTO_CREATE_METADATA] Salvando metadata:`,
          messageMetadata,
        );

        // Salvar resposta da IA com metadata
        aiMessage = await chatRepository.ChatMessageRepository.create({
          chatSessionId: session.id,
          senderRole: "ai",
          content: aiContent,
          status: "ok",
          metadata: messageMetadata,
        });

        console.log("‚úÖ [AUTO_CREATE] Resposta da IA processada");
      } catch (aiError) {
        console.error("‚ö†Ô∏è [AUTO_CREATE] Erro ao processar com IA:", aiError);

        // Salvar mensagem de erro da IA
        aiMessage = await chatRepository.ChatMessageRepository.create({
          chatSessionId: session.id,
          senderRole: "ai",
          content: `Erro: ${aiError instanceof Error ? aiError.message : "Erro ao processar mensagem"}`,
          status: "error",
        });
      }
    }

    console.log(
      "üéâ [AUTO_CREATE] Sess√£o criada com sucesso!",
      session.id,
      "- T√≠tulo:",
      title,
    );

    return {
      session,
      userMessage,
      aiMessage,
    };
  } catch (error: any) {
    console.error("‚ùå [AUTO_CREATE] Erro:", error);

    if (error instanceof TRPCError) {
      throw error;
    }

    throw new TRPCError({
      code: "INTERNAL_SERVER_ERROR",
      message: error.message || "Erro ao criar sess√£o autom√°tica",
    });
  }
}
