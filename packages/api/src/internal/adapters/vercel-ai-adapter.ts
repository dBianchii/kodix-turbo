import { createOpenAI } from "@ai-sdk/openai";
import { streamText } from "ai";

import { chatAppId } from "@kdx/shared";

import type {
  ChatStreamParams,
  ChatStreamResponse,
} from "../types/ai/vercel-adapter.types";
import { AiStudioService } from "../services/ai-studio.service";

export class VercelAIAdapter {
  /**
   * üöÄ REAL IMPLEMENTATION: Usa Vercel AI SDK de verdade
   * Com fallback para mock em caso de erro
   */
  async streamResponse(params: ChatStreamParams): Promise<ChatStreamResponse> {
    console.log("üß™ [VERCEL-ADAPTER] Iniciando stream com SDK real...");

    // üé≠ DETEC√á√ÉO DE MOCK MODE: Se modelId for "mock-model", usar mock direto
    if (params.modelId === "mock-model") {
      console.log(
        "üé≠ [VERCEL-ADAPTER] Mock mode detectado - usando mock direto",
      );
      return this.getMockResponse(params, new Error("Mock mode ativado"));
    }

    try {
      // 1. Converter par√¢metros para formato Vercel AI SDK
      const vercelParams = this.adaptInputParams(params);
      console.log("‚úÖ [VERCEL-ADAPTER] Par√¢metros adaptados:", {
        messagesCount: vercelParams.messages.length,
        temperature: vercelParams.temperature,
        maxTokens: vercelParams.maxTokens,
      });

      // 2. Obter modelo configurado
      const model = await this.getVercelModel(params.modelId, params.teamId);
      console.log("‚úÖ [VERCEL-ADAPTER] Modelo obtido:", typeof model);

      // 3. USAR VERCEL AI SDK PELA PRIMEIRA VEZ! üéâ
      console.log(
        "üöÄ [VERCEL-ADAPTER] Chamando streamText do Vercel AI SDK...",
      );
      const result = await streamText({
        model,
        messages: vercelParams.messages,
        temperature: vercelParams.temperature,
        maxTokens: vercelParams.maxTokens,
      });

      console.log("‚úÖ [VERCEL-ADAPTER] streamText executado com sucesso");

      // 4. Adaptar resposta para formato atual
      return this.adaptResponse(result);
    } catch (error) {
      console.error("üî¥ [VERCEL-ADAPTER] Erro no SDK, usando fallback:", error);

      // FALLBACK PARA MOCK (seguran√ßa m√°xima)
      return this.getMockResponse(params, error);
    }
  }

  /**
   * Converte par√¢metros atuais para Vercel AI SDK
   */
  private adaptInputParams(params: ChatStreamParams) {
    const messages = params.messages.map((msg) => {
      // Mapear roles corretamente
      let role: "user" | "assistant" | "system";

      if (msg.senderRole === "user") {
        role = "user";
      } else if (msg.senderRole === "ai" || msg.senderRole === "assistant") {
        role = "assistant";
      } else if (msg.senderRole === "system") {
        role = "system";
      } else {
        // Fallback para roles desconhecidos
        role = "user";
      }

      return {
        role,
        content: msg.content,
      };
    });

    console.log("üîÑ [VERCEL-ADAPTER] Mensagens convertidas:", {
      total: messages.length,
      roles: messages.map((m) => m.role),
    });

    return {
      messages,
      temperature: params.temperature || 0.7,
      maxTokens: params.maxTokens || 4000,
    };
  }

  /**
   * Mapeia modelos do AI Studio para Vercel AI SDK
   * FASE 1: Apenas OpenAI (conservador)
   */
  private async getVercelModel(modelId: string, teamId: string) {
    console.log("üîç [VERCEL-ADAPTER] Buscando modelo via AiStudioService...");

    // Buscar modelo via Service Layer (mesma forma que o sistema atual)
    const modelConfig = await AiStudioService.getModelById({
      modelId,
      teamId,
      requestingApp: chatAppId,
    });

    console.log("‚úÖ [VERCEL-ADAPTER] Modelo encontrado:", {
      name: modelConfig.name,
      provider: modelConfig.provider.name,
    });

    // Buscar token do provider
    const providerToken = await AiStudioService.getProviderToken({
      providerId: modelConfig.providerId,
      teamId,
      requestingApp: chatAppId,
    });

    console.log(
      "‚úÖ [VERCEL-ADAPTER] Token encontrado para provider:",
      modelConfig.provider.name,
    );

    // FASE 1: Suporte apenas OpenAI (conservador)
    const providerName = modelConfig.provider.name.toLowerCase();

    if (providerName === "openai") {
      const modelName =
        (modelConfig.config as any)?.version || modelConfig.name;
      console.log(
        "üîß [VERCEL-ADAPTER] Configurando OpenAI com modelo:",
        modelName,
      );

      // Criar inst√¢ncia do provider OpenAI
      const openaiProvider = createOpenAI({
        apiKey: providerToken.token,
        baseURL: modelConfig.provider.baseUrl || undefined,
      });

      // Retornar modelo espec√≠fico
      return openaiProvider(modelName);
    } else {
      throw new Error(
        `üöß Provider ${modelConfig.provider.name} ainda n√£o suportado no adapter. Apenas OpenAI na Fase 1.`,
      );
    }
  }

  /**
   * Adapta resposta do Vercel AI SDK para formato atual
   */
  private adaptResponse(vercelResult: any): ChatStreamResponse {
    console.log("üîÑ [VERCEL-ADAPTER] Adaptando resposta do SDK...");

    // Converter stream do Vercel AI SDK para formato atual
    const stream = new ReadableStream({
      async start(controller) {
        try {
          console.log("üì° [VERCEL-ADAPTER] Iniciando leitura do textStream...");
          let chunkCount = 0;

          for await (const chunk of vercelResult.textStream) {
            chunkCount++;
            if (chunkCount === 1) {
              console.log("‚úÖ [VERCEL-ADAPTER] Primeiro chunk recebido");
            }
            controller.enqueue(new TextEncoder().encode(chunk));
          }

          console.log(
            `‚úÖ [VERCEL-ADAPTER] Stream finalizado. Total chunks: ${chunkCount}`,
          );
        } catch (streamError) {
          console.error("üî¥ [VERCEL-ADAPTER] Erro no stream:", streamError);
          controller.enqueue(
            new TextEncoder().encode(
              `\n\n[Erro no stream: ${streamError instanceof Error ? streamError.message : "Erro desconhecido"}]`,
            ),
          );
        } finally {
          controller.close();
        }
      },
    });

    return {
      stream,
      metadata: {
        model: vercelResult.response?.modelId || "vercel-sdk-model",
        usage: vercelResult.usage || null,
        finishReason: vercelResult.finishReason || "stop",
      },
    };
  }

  /**
   * Fallback para mock em caso de erro (seguran√ßa)
   */
  private getMockResponse(
    params: ChatStreamParams,
    originalError: any,
  ): ChatStreamResponse {
    const errorMessage =
      originalError instanceof Error
        ? originalError.message
        : "Erro desconhecido";

    const isMockMode = params.modelId === "mock-model";

    if (isMockMode) {
      console.log("üé≠ [VERCEL-ADAPTER] Gerando resposta mock intencional");
    } else {
      console.log("üé≠ [VERCEL-ADAPTER] Usando resposta mock como fallback");
    }

    const stream = new ReadableStream({
      start(controller) {
        const mockContent = isMockMode
          ? `üé≠ **Mock Adapter - Modo Teste**\n\n` +
            `‚úÖ Vercel AI SDK Adapter est√° funcionando!\n\n` +
            `**Par√¢metros processados:**\n` +
            `- Modelo: ${params.modelId}\n` +
            `- Team: ${params.teamId}\n` +
            `- Mensagens: ${params.messages.length}\n` +
            `- Temperature: ${params.temperature || 0.7}\n` +
            `- Max Tokens: ${params.maxTokens || 4000}\n\n` +
            `**Status:** Mock mode ativo - adapter funcionando corretamente!\n\n` +
            `Esta resposta confirma que a estrutura do Vercel AI SDK est√° implementada e funcional.`
          : `üé≠ **Mock Adapter - Fallback Seguro**\n\n` +
            `Vercel AI SDK falhou, usando mock como fallback seguro.\n\n` +
            `**Erro original:** ${errorMessage}\n\n` +
            `**Par√¢metros recebidos:**\n` +
            `- Modelo: ${params.modelId}\n` +
            `- Team: ${params.teamId}\n` +
            `- Mensagens: ${params.messages.length}\n\n` +
            `Esta √© uma resposta simulada para garantir que o sistema continue funcionando.`;

        controller.enqueue(new TextEncoder().encode(mockContent));
        controller.close();
      },
    });

    return {
      stream,
      metadata: {
        model: isMockMode ? "mock-intentional" : "mock-fallback",
        usage: null,
        finishReason: "stop",
        error: isMockMode ? undefined : errorMessage,
      },
    };
  }
}
